<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="frédéric clavert" />
  <title>negotiating the past</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="negotiating_the_past_files/reveal.js-4.2.1/dist/reset.css">
  <link rel="stylesheet" href="negotiating_the_past_files/reveal.js-4.2.1/dist/reveal.css">

  <style type="text/css">
    /* CSS from pandoc style.html() */
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
                  </style>

  <link rel="stylesheet" href="negotiating_the_past_files/reveal.js-4.2.1/dist/theme/simple.css" id="theme">


  <style type="text/css">
  /* some tweaks to reveal css */
    .reveal h1 { font-size: 2.0em; }
    .reveal h2 { font-size: 1.5em;  }
    .reveal h3 { font-size: 1.25em;	}
    .reveal h4 { font-size: 1em;	}

    .reveal .slides>section,
    .reveal .slides>section>section {
      padding: 0px 0px;
    }
    .reveal table {
      border-width: 1px;
      border-spacing: 2px;
      border-style: dotted;
      border-color: gray;
      border-collapse: collapse;
      font-size: 0.7em;
    }

    .reveal table th {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      font-weight: bold;
      border-style: dotted;
      border-color: gray;
    }

    .reveal table td {
      border-width: 1px;
      padding-left: 10px;
      padding-right: 25px;
      border-style: dotted;
      border-color: gray;
    }
  </style>

    <script src="negotiating_the_past_files/header-attrs-2.29/header-attrs.js"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">negotiating the past</h1>
  <h2 class="author"><a href="https://inactinique.net">frédéric
clavert</a></h2>
  <h3 class="date">14/05/2025</h3>
</section>

<section id="section" class="title-slide slide level1"
data-background-color="#000000">
<h1 data-background-color="#000000"></h1>
<p>historical representations / artificial intelligence / collective
memory</p>
<!-- <small>prompt: Draw an image representing what is collective memory</small> -->
<aside class="notes">
<p>This talk will explore the intersection of historical
representations, artificial intelligence, and collective memory.</p>
<p>Negotiating the past? In a way, we – as a collective body, a society
– are “negotiating” the past in different spaces: committees that define
which historical knowledge students should learn, for instance.</p>
<!-- In more dramatic circumstances, Truth and Reconciliation commissions, like in South Africa, are also some sort of spaces to negotiate the past, or, in this precise case, to negotiate which reparation should be defined from social groups that suffered from conflicting past. But to define those reparations, there should first be an agreement, or at least some sort of non opposition to the way the commission is seeing the past. -->
<p>Concerning an event that is more linked to my very old doctoral
research, the international military trial in Nurenberg was a space of
negotation of the past – with the very caricatural position on the Katyn
massacre for instance.</p>
<p>Let’s go back to today’s subject. I’ll try to examine how users
“negotiate” with AI systems to express their conceptions of the past,
how these interactions can reveal tensions between user expectations and
AI-embedded historical patterns, and how to instert this into memory
studies. My argument is that spaces of negotiations about the past are
already existing and that generative AI platforms are adding one, that
has its own specificities.</p>
<ul>
<li>I’ll try first to show how LLMs encode historical perspectives,
trying to set up a theoretical framework.</li>
<li>I’ll then show how I am using myself AI and LLMs to get prompts
containing reference(s) to the past, to set up a corpus of prompts to
analyse, and how I do analyse them.</li>
<li>I’ll then comment the analyses before concluding on chatbots /
diffusion systems as new spaces for negotiation on the diverse ways we
collectively see the past.</li>
</ul>
</aside>
</section>

<section>
<section id="an-attempt-at-a-theoretical-framework"
class="title-slide slide level1" data-background-color="#000000">
<h1 data-background-color="#000000">an attempt at a theoretical
framework</h1>

</section>
<section id="chatbots-as-medium-of-memory" class="slide level2"
data-background-color="#000000">
<h2 data-background-color="#000000">chatbots as medium of memory</h2>
<p><img data-src="img/invite.gif" /></p>
<aside class="notes">
<p>I define here ‘chatbots’ as generative AI platform for wide
audiences, made of an interface that encourages users to enter a prompt,
in ordre to generate text, images, videos, etc. Those platforms are more
and more multimodal – inputs can be texts or images, as well as outputs.
So we consider here chatbots that are based on diffusion systems or
large language models.</p>
<p>We also consider chatbots, and not directly their underlying engines
(large language models, or diffusion systems), because chatbots have
additional layer of alignments and fine tuning.</p>
<p>For instance, DeepSeek – the online chatbot – is well known to refuse
to answer a question on the Tiananmen massacre. But if you use one of
the DeepSeek R1 models directly on your computer (I’ve tested the 7b
parameters one), it does not refuse to answer questions on the Tiananmen
massacre, though it clearly indicates in its reasoning that it should be
respecting the chinese law and sensibility on the subject.</p>
<p>This Tiananmen example brings us back to memory: in those additional
layers of alignment and fine tuning that are made to allow wide
audiences to use easily chatbots, there are additional views on the past
that are modified or embedded.</p>
<p>As a consequence, those chatbots can be considered as medium of
memory. I use here Astrid Erll’s work (Memory in Culture, 2011) where
she defines a medium of memory as « “constructs versions of a past
reality” and plays a role “in the encoding and decoding [Stuart Hall,
1980.] of that which is (to be) remembered.” » (p. 120ff). For Erll,
medium of memory performs several functions:</p>
<ul>
<li>they store information</li>
<li>they allow a form of ciruclation of this information</li>
<li>they can also be collective memory triggers</li>
</ul>
<p>For instance: a monument or a statute can be a medium of memory:</p>
<ul>
<li>they in some ways allow for the storage of information,</li>
<li>though it is not their main function, they allow this information to
circulate – basically they are a message to people passing by,</li>
<li>they are trigger of collective memory: see French Monuments aux
morts.</li>
</ul>
<p>Let’s go back to chatbots:</p>
<ul>
<li>chatbots are based on models that are <strong>storing
information</strong>. Models are sets of parameters deduced from a
training phase, parameters that contain patterns based on training
datasets. In this sense they can be seen as storage of information,
including information on the historical past,</li>
<li>chatbots allow a form of circulation of information: that’s why they
are here, or at least why you pay to use them. Nevertheless, their
stochastic ways to restitute information, that does not include any
sense of truth, of facts, that can lead to hallucinations, should be
carefully considered.</li>
<li>chatbots are also triggers of collective memory. That’s their
interface, based on, often, a single box where users can type questions.
Studying the past, in a professional way or a more amateur one, is all
about asking questions and trying to find ways – sources and their
analysis – to answer them. prompts are huge incentives / triggers to
query the past, even if they are not only about that, of course.</li>
</ul>
<p>So, if we consider chatbots as medium of memory, we should also try
understanding how they embed views on the past.</p>
</aside>
</section>
<section id="chatbots-and-historical-patterns" class="slide level2"
data-background-color="#000000">
<h2 data-background-color="#000000">chatbots and historical
patterns</h2>
<p><img data-src="img/fractal.gif" /></p>
<aside class="notes">
<p>We’ll take the example of text generation through LLMs. LLMs are
statistical pattern recognizers. They can use patterns to “generate the
next word” thanks to their training phase, based on training dataset.
This training dataset is not well known, but we should consider it as
historians: it is an archive, it is made of historical records. But as
all historical records, they contain specific views of the past – I
could speak of ‘biases’, but I think this word is really not pertinent
here. And, the way the training phase is embedding those views of the
past into the model is not at all the way we are dealing with historical
records as historians. There’s no critical appraisal of those historical
records. What matters are patterns, probabilities.</p>
<p>Hence the famouse article of Bender et al. on stochastic parrots.
Applying probabilities, patterns to the historical past is not at all
the way we are working as historians to try to reconstitute the
past.</p>
</aside>
</section>
<section id="aligning-the-past" class="slide level2"
data-background-color="#000000">
<h2 data-background-color="#000000">aligning the past</h2>
<p><img data-src="img/leopold.png" /></p>
<aside class="notes">
<p>We should also look at the alignment, feedback (with or without
humans in the loop), fine-tuning and any other operations that can be
performed to create the chatbot itself. Alignment, for instance, is
supposed to ensure that the answers of a chatbot, whatever their
modalities, fits with the value of a society: as one of the article we
have published in the <em>Memory Studies Review</em> special issue we
coordinated with Sarah Gensburger argues, in the end, “alignment” ensure
also that the firm publishing the chatbot gets money from it, which
probably means that the answers must please a large number of users (and
also the firm’s shareholders).</p>
<p>Of course, this alignement is obvious for chatbots that are published
in an authoritarian state. We have seen the DeepSeek Tienanmen example
before. I could also give the example of ruDALLE, a russian image
generation platform, that will send back an image of flowers to all
prompts mentioning something related to Ukraine.</p>
<p>Here is a different example: midjourney, as well as DALL.E, have
filters that encourage the representation of minorities on the images
they produce, which is obviously of good intent. But this can lead to
abberations. In this image, the aberration is to represent Leopold III
as a black king colonizing Congo.</p>
</aside>
</section>
<section id="the-latent-space-of-the-past" class="slide level2"
data-background-color="#000000">
<h2 data-background-color="#000000">the latent space of the past</h2>
<p><img data-src="img/zeus_vectors.png" /></p>
<aside class="notes">
<p>Still taking the examples of LLMs, historical concepts are encoded as
points in a multidimensional space, made of word embedings that capture
semantic relationships. Then the relationships between historical
events, figures, and concepts are captured in the distances and
directions between these vectors. Temporal relationships are encoded –
or we can suppose so – in semantic proximity and cultural associations,
including related to memory, they are embedded in language patterns.
This multidimensional space is ‘compressed’ in a latent space that
captures all those relationships and, when activated by a prompt
containing a reference to the past, will send back an answer. This
latent space reflect collective memory patterns from the training
corpus.</p>
see Alban Leveau-Vallier’s talk and book
</aside>
<!-- À fusionner avec l'une des dias plus haut.

## collective memory and chatbots {background-color=#000000}

<aside class=notes>

Hence can we consider that: 

- LLMs or Diffusion models as 'repositories' of digitized collective memory
- Training data selection can be viewed as some sort of memory politics
- The "averaged" nature of AI-generated historical narratives
- Absence of contested memory in statistical consensus

From a memory studies perspective, LLMs function as repositories of digitized collective memory. The selection of training data constitutes a form of memory politics, determining which historical perspectives are included or excluded. The statistical nature of these models produces "averaged" historical narratives that often elide contestation and complexity.

See the special issue we co-edited with Sarah Gensburger, out last december. Several articles insist on the "average" nature of LLMs but also the capitalistic nature of chatbots and generative AI systems: they must give the most 'popular' or 'pleasant' version of an answer to a prompt, including the past. 

</aside>
-->
</section>
<section id="chatbots-as-frameworks" class="slide level2"
data-background-color="#000000">
<h2 data-background-color="#000000">chatbots as frameworks</h2>
<p><img src="img/hakbwachs.png" width=30% /></p>
<aside class="notes">
<p>So, chatbots (and LLMs / Diffusion models behind them) are palying
their role of encoding / decoding what is to be remembered through this
“latent space of the past” – an expression that should be considered
here as a metaphore. This latent space is a specificity of chatbots
considered as medium of memory.</p>
<p>One of the functions of memory media is also to play their role as
media frameworks. Their affordances, including anthropomorphism, the
ways they are structured, are framing what users can expect from those
systems.</p>
<p>Beyond that, we will here try to explain in which ways they are also
memory frameworks, in the meaning Halbwachs gave to this word, and I
think this is a consequence of the fact that chatbots are memory
media.</p>
<p>What does Halbwachs mean by “memory frameworks”: basically that you
can not understand how an individual remembers if you do not consider
the social group in which they are living, including how this social
group is structured. It is this relationship to the group that frame the
individual memory, as the group will have its own view on the past, view
that is a consequence of the social relationships between individuals of
the group, and of the insertion of this group in a wider society.</p>
</aside>
</section></section>
<section>
<section id="empirical-approach" class="title-slide slide level1"
data-background-color="#000000">
<h1 data-background-color="#000000">empirical approach</h1>
<aside class=notes>
<p>So my first part was an attempt – probably still too drafty – of
theoricizing what are chatbots in terms of collective or cultural
memory.</p>
<p>But as a historian, I have difficulties not to link this to primary
sources and to an empirical approach.</p>
<p>What I am trying to do here is to look at prompts and how users are –
this is the core of my argument – negotiating with the chatbot to get
the view of the past they wish. That implies that users may have
different prompting strategies, that their prompts evolve while
discussing – I call this discussion part ‘stochastic maieutics’ – with
the chatbot. I will try to use at the same time close and distant
reading of prompts.</p>
<p>I am using here only texts – whereas today, images can be some sort
of prompts too.</p>
<p>I am using prompts that are published open access. THey usually comes
from open source communities, or are available online on website such as
<a href="https://lexica.art">lexica.art</a> that offer an API to get
prompts. The result is that those prompts are mostly coming from Stable
Diffusion-based platforms, as Stable Diffusion is open source: those
prompts are designed to generate image, not text. There is no databases
of prompts, to my knowledge, designed to generate texts. Nevertheless,
HuggingFace is vast, and I may have missed something.</p>
<aside>
</section>
<section id="what-is-a-reference-to-the-past" class="slide level2"
data-background-color="#000000">
<h2 data-background-color="#000000">what is a reference to the
past?</h2>
<div style="font-size:11pt;">
<blockquote>
<p>army of the european union invades budapest 2 0 2 2, highly detailed
painting, digital painting, artstation, concept art</p>
</blockquote>
<blockquote>
<p>army of the european union fighting on the streets of budapest 2 0 2
2, highly detailed illustration for time magazine cover art</p>
</blockquote>
<blockquote>
<p>army of the european union with tanks fighting on the streets of
budapest 2 0 2 2, highly detailed oil painting</p>
</blockquote>
</div>
<aside class="notes">
<p>So the aim is to build a corpus of prompts containing some sort of
references to the past and to analyse it. There is a set of
methodological issues to take into account:</p>
<ul>
<li>unless I want to study a specific event or individual, a keyword
approach is meaningless.</li>
<li>how to define a historical reference? All the more when they are
implicit?</li>
</ul>
<p>Let’s take an example: the three prompts here are not refering
explicitly to the past. But just use the prompt for a search on an image
search engine: all the results are historical. We have here an implicit
reference, probably to the hungarian revolution of 1956, that was put to
an end by a soviet intervention.</p>
<p>So how to build a robust identification strategy?</p>
<p>I have tried several strategies:</p>
<ul>
<li>web scrapping on focused keywords (European Union): limited corpus,
humanly readable, some distant reading,</li>
<li>word embeddings: keywords as seeds, then word embeddings based on an
unfiltered corpus of 10 millions (kreia) or a sample,</li>
<li>good old iramuteq (topic-modeling like approach) on this same
unfiltered corpus of 10 millions prompts – analysis is still
ongoing,</li>
<li>use of LLMs (best results if you do not want to start with
keywords):
<ul>
<li>a local one (too this same unfiltered corpus of ressource intensive
for my computer),</li>
<li>APIs from Claude (9% of is prompts) of from ChatGPT (more
restrictive).</li>
</ul></li>
</ul>
<p>Though not perfect – the implicit part is not fully taken into
account and some prompts are refering to the past, but not the
historical past – it’s the use of Claude that worked the best, if you
don’t want to use keywords. I must precise that for now I did not do any
benchmarking, which is an obvious weakness, that I plan to adress.</p>
<p>In this prez, I’m using two datasts: one from web scrapping and the
other one is a sample of the 10 millions prompts Kreia corpus, sample
that was filtered trhoug the use of Claude’s API. Let’s have a look on
how I have used Claude.</p>
</aside>
</section>
<section id="prompting-to-find-references-to-the-past"
class="slide level2" data-background-color="#000000">
<h2 data-background-color="#000000">prompting to find references to the
past</h2>
<p><img src="img/prompt.png" width=80% /></p>
<aside class="notes">
<p>Part of the analyses i’ll discussed later here is based on a prompts
corpus that was created using the Claude API on one side and web
scrapping on the keyword ‘european union’ on the other side. Let’s focus
a bit on Claude: this prompt was co-written with Claude to help Claude
reasoning for each item of my corpus. It’s a mixed approach: reasoning
and personae, the prompt being enginered through a negociation between
me and Claude (the chatbot).</p>
<p>What is missing here is that I could add as an instruction to explain
why it decided for a yes or for a no.</p>
<p>Once I have a corpus – or, here, two corpora – I can perform
analyses. I will go from distant to close readings. Distant reading here
is at the same time used to start the interpretation and as a search
tools for more refined analyses, including analyses of precise prompts
or series of prompts.</p>
</aside>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<p><img src="img/historical_yes_dendrogramme_1.png" /></p>
<aside class="notes">
<p>This is a dataviz obtained with iramuteq. Iramuteq performs something
that looks like topi modelling, but it is not bag of words. The words
that you see are in fact the most representative words of clusters of
prompts.</p>
<p>It is a dataviz that should still be reworked, in the sense that the
settings used here should be refined to make the difference between two
parts of the usual prompt: the part with content, the part with the
style.</p>
<p>Nevertheless, there’s some past everywhere here. In the style,
obviously, but also in the content.</p>
<p>And most references to the past, if they are not arty, are to wars
(cluster 17), or propanganda wars (cluster 8). And most of them are
somehow linked to the present news (trump, propaganda, soviet, macron in
the same cluster for instance).</p>
</aside>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<p><img src="img/eu_dendrogramme_1.png" width="70%" /></p>
<aside class="notes">
<p>Same dataviz, but with the webscrapped from lexica corpus, based on
the ‘european union’ keyword. The lexica API has several sorts of
keyword search and here is a keyword search that is based on word
embeddings: it’s not strictly a keyword search, more a ‘thematic’
search.</p>
<p>What you see here is interesting, because it’s a lot linking the
European Union and Europe themes to all sorts of empire themes and to
some sort of medievalism. We also find back the ‘propaganda’ cluster –
probably because those prompts were produced in the wake of the
agression against Ukraine.</p>
</aside>
</section></section>
<section>
<section id="negotiating-the-past" class="title-slide slide level1"
data-background-color="#000000">
<h1 data-background-color="#000000">negotiating the past</h1>
<aside class="notes">
<p>Those distant reading analyses are interesting, but confirm more than
discover: in a way, we expect Europe to be linked to the concept of
empire, we expect that lots of references to the past are linked to the
‘style’ part of a prompt.</p>
<p>But those distant readings allow us also to go back to specific
prompts. By looking at prompts that are very similar, we can trace the
evolution of a prompt written / re-written several times by a user. And
it’s here, that we can see that gen AI platforms, seen as frameworks,
encourage users to negotiate with the machine the past they want to see
or read.</p>
<p>This negotiation can be seen as a confrontation between several kinds
of collective memories: the one that are embedded in the genAI platform
and that comes from the way the LLM or diffusion system was trained and
from the corpus it was trained on; the collective memory of the group
the individual belongs too; the individuals own vision of the past.</p>
<p>I’ll give two examples.</p>
</aside>
</section>
<section id="section-3" class="slide level2"
data-background-color="#000000">
<h2 data-background-color="#000000"></h2>
<p><img src="img/macron-leyen-knights2.jpg" width="50%" /></p>
<blockquote>
<p><small>Ursula von Der Leyne [sic] and Emmanuel Macron, Peter [sic]
Pavel in the image of knights of the round table</small></p>
</blockquote>
<aside class="notes">
<p>I have showed before prompts that relate to the hungarian revolution
– unfortunately I do not have the images.</p>
<p>Here is another example. Several prompts of this kind were written,
with some differences, but results that are very similar.</p>
<p>As you can see, we can consider there are several references to the
past here.</p>
<ul>
<li>the reference to a myth, the knight of the round table, that is
supposed to be medieval, but that is illustrated by an image that looks
more Renaissance than medieval - which gives a hint on how collectively
we see the Middle Ages.</li>
<li>as this image is from 2023, it is also a way to see how the memory
of some politicians is being built – and obviously, Petr Pavel’s memory
outside of the czesh republic is not very well built…</li>
<li>we could say also that Macron is slightly more recongizable than von
der Leyen (who looks like an average 50s-60s-ish european woman), but
the fault in the prompt might induce that.</li>
</ul>
<p>The user never managed to have Petr Pavel on their images.</p>
</aside>
</section>
<section id="section-4" class="slide level2"
data-background-color="#000000">
<h2 data-background-color="#000000"></h2>
<p><img src="img/biden.png" width="50%" /></p>
<blockquote>
<p><small>joe biden doing a nazi salute, in front of brandenburger tor.
huge nazi crowd in front of him. face of joe biden is clearly visible.
canon eos r 3, f / 1. 4, iso 1 6 0 0, 1 / 8 0 s, 8 k, raw,
grainy</small></p>
</blockquote>
<aside class="notes">
<p>This is a striking example of negotition with a machine to get
something in the present with references to the past that serves as an
ideological reading of the present.</p>
<p>The user never got what they wanted. Biden doing a nazi salute in
front of nazis – that just does not exist. Nevertheless, the prompt
activated patterns from the second world war and maybe from the cold
war.</p>
<p>I may be overinterpreting, but I can see influence, of course, from
nazi footages, but also from De Gaulle in August 1944 (26) in the Champs
Elysées, Kennedy in front of the Brandenburger Tor.</p>
<p>The political goal of the prompt writer here is partly in failure:
this political goal is confronted with the collective memory of the
second world war, that are preeminent over their views.</p>
</aside>
</section></section>
<section>
<section id="conclusion" class="title-slide slide level1"
data-background-color="#000000">
<h1 data-background-color="#000000">conclusion</h1>
<aside class="notes">
<p>Chatbots function as media of memory that store, circulate, and
trigger collective memories. They are medium of memory because AI models
encode historical perspectives - metaphorically, we could call that the
collective memory latent space – that reflects collective memory
patterns from training corpora. On top of the training, process,
alignment and fine-tuning embed specific views of the past (e.g.,
DeepSeek/Tiananmen, minority representations). In this sense, we can
also consider chatbots as frameworks.</p>
<p>When we look at users’ prompt that have references to the past, we
see how users are writing those references to the past, but also how
they negotiate with gen AI systems to obtain their desired vision of the
past, creating a confrontation between different collective memories,
but also, often, their vision of the present.</p>
<p>This negotiation reveals tensions between user expectations and
historical representations (frameworks) embedded in AI.</p>
</aside>
</section>
<section id="whats-next" class="slide level2"
data-background-color="#000000">
<h2 data-background-color="#000000">what’s next</h2>
<p><img src="img/04_DeGaulle.jpg" width=30% /></p>
<blockquote>
<p><small>“De Gaulle bronze sur une plage de Normandie”</small></p>
</blockquote>
<aside class="notes">
<p>I have tried to prove all that with primary sources, in other words
prompts. It’s far from perfect. My corpus is not good enough, it must be
reinforced, better tailored to my needs. It is possible in different
ways, that I can detail by answering questions.</p>
<p>Beyond this, I think we should remid something quite important about
LLMs and Diffusion System: they are the products of artefacts from the
past (the training dataset), they are producing primary sources
(prompts, images, texts), and they are triggers of collective
memory.</p>
<p>In this sense, GenAI systems are fundmentaly historical products.</p>
</aside>
</section>
<section id="thank-you" class="slide level2"
data-background-color="#000000">
<h2 data-background-color="#000000">thank you</h2>
</section></section>
    </div>
  </div>

  <script src="negotiating_the_past_files/reveal.js-4.2.1/dist/reveal.js"></script>
  
  <!-- reveal.js plugins -->
  <script src="negotiating_the_past_files/reveal.js-4.2.1/plugin/notes/notes.js"></script>
  
  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,
        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,
        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',
        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',
        // Display a presentation progress bar
        progress: true,
        // Can be used to limit the contexts in which the slide number appears
        showSlideNumber: 'all',
        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,
        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,
        // Push each slide change to the browser history
        // Implies `hash: true`
        history: true,
        // Enable keyboard shortcuts for navigation
        keyboard: true,
        // Enable the slide overview mode
        overview: true,
        // Vertical centering of slides
        center: true,
        // Enables touch navigation on devices with touch input
        touch: true,
        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',
        // Turns fragments on and off globally
        fragments: true,
        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,
        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,
        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,
        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,
        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,
        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,
        // Stop auto-sliding after user input
        autoSlideStoppable: true,
        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,
        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Transition style
        transition: 'convex', // none/fade/slide/convex/concave/zoom
        // Transition speed
        transitionSpeed: 'default', // default/fast/slow
        // Transition style for full page slide backgrounds
        backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom
        // Number of slides away from the current that are visible
        viewDistance: 3,
        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,
        // The display mode that will be used to show slides
        display: 'block',
        // Hide cursor if inactive
        hideInactiveCursor: true,
        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,



        // Optional reveal.js plugins
        plugins: [
          RevealNotes,
        ]
      });

    </script>
  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

<script>
  (function() {
    if (window.jQuery) {
      Reveal.addEventListener( 'slidechanged', function(event) {  
        window.jQuery(event.previousSlide).trigger('hidden');
        window.jQuery(event.currentSlide).trigger('shown');
      });
    }
  })();
</script>


  </body>
</html>
