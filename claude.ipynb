{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncks0uTDzzbr"
      },
      "source": [
        "# negotiating the past"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAMm3ta9zzbt"
      },
      "source": [
        "We'll use ollama and llama3 7b to run some test.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmRnFRyxzzb1"
      },
      "source": [
        "We load the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrCaiB6xzzb1",
        "outputId": "3be594f9-f6ce-4d62-9e35-6e942556cb4f"
      },
      "outputs": [],
      "source": [
        "!python --version\n",
        "%pip install umap-learn\n",
        "%pip install gensim\n",
        "%pip install nltk\n",
        "%pip install pandas\n",
        "%pip install anthropic\n",
        "%pip install ipywidgets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaKgggqHzzb2"
      },
      "source": [
        "## Creating a Historical Prompt Dataset with Claude"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import libraries and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX5tD0evzzb3",
        "outputId": "94ffeb73-1938-41ad-eaab-954e7c334b0b"
      },
      "outputs": [],
      "source": [
        "# Import libraries and setup\n",
        "import anthropic\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "import os\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm  # Note the use of tqdm.notebook for better display in Jupyter\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import json\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(f\"past_reference_detection_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(\"past_reference_detector\")\n",
        "\n",
        "# Create results directory\n",
        "os.makedirs(\"data/results_claude\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load API key securely from a separate JSON file (recommended)\n",
        "def load_api_key():\n",
        "    try:\n",
        "        # Try to load from config file\n",
        "        with open('api_config.json') as f:\n",
        "            config = json.load(f)\n",
        "            return config.get('CLAUDE_API_KEY')\n",
        "    except FileNotFoundError:\n",
        "        # If file doesn't exist, check environment variables\n",
        "        api_key = os.environ.get(\"CLAUDE_API_KEY\")\n",
        "        if api_key:\n",
        "            return api_key\n",
        "        else:\n",
        "            # If not found, prompt user (only in notebook environment)\n",
        "            from IPython.display import display\n",
        "            from ipywidgets import widgets\n",
        "            \n",
        "            password = widgets.Password(\n",
        "                description='Claude API Key:',\n",
        "                style={'description_width': 'initial'},\n",
        "                layout={'width': '50%'}\n",
        "            )\n",
        "            display(password)\n",
        "            \n",
        "            # This will allow the user to enter their API key securely\n",
        "            # The entered key will be accessible via password.value in subsequent cells\n",
        "            return None\n",
        "\n",
        "# Create a JSON file for the API key that you won't commit to GitHub\n",
        "# (Run this cell ONCE, then delete or comment it out)\n",
        "\"\"\"\n",
        "import json\n",
        "with open('api_config.json', 'w') as f:\n",
        "    json.dump({'CLAUDE_API_KEY': 'your_api_key_here'}, f)\n",
        "    \n",
        "print(\"Created api_config.json - add this file to .gitignore!\")\n",
        "\n",
        "# Create .gitignore if it doesn't exist\n",
        "if not os.path.exists('.gitignore'):\n",
        "    with open('.gitignore', 'w') as f:\n",
        "        f.write(\"api_config.json\\n.ipynb_checkpoints/\\n__pycache__/\\n*.pyc\\nresults/\\nlogs/\\n\")\n",
        "    print(\"Created .gitignore file\")\n",
        "\"\"\"\n",
        "\n",
        "# Load the API key\n",
        "api_key = load_api_key()\n",
        "if api_key and api_key.startswith(\"sk-\"):\n",
        "    print(\"✅ API key loaded successfully\")\n",
        "    # Initialize Claude client\n",
        "    client = anthropic.Anthropic(api_key=api_key)\n",
        "else:\n",
        "    print(\"❌ API key not found or not valid format\")\n",
        "    # If using the widget approach, you'll need to get the value in the next cell\n",
        "    # client = anthropic.Anthropic(api_key=password.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define your prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Third cell - Helper functions\n",
        "\n",
        "system_prompt = \"\"\"As a panel of three historians with different specializations (international history, global history, and European history), analyze the following prompt to determine if it contains an implicit or explicit reference to the past.\n",
        "\n",
        "Each historian should consider:\n",
        "\n",
        "1. EXPLICIT REFERENCES: Clear temporal markers (yesterday, last week, previously), historical events, periods, or figures, or mentions of things that happened in the past.\n",
        "\n",
        "2. IMPLICIT REFERENCES: Subtle indications of past time frames, comparative language suggesting change over time, or references to completed actions or states that are no longer current.\n",
        "\n",
        "3. CONTEXTUAL CLUES: Words implying memory, reflection, or nostalgia; verbs in past tense that indicate historical events rather than hypotheticals.\n",
        "\n",
        "4. DOMAIN-SPECIFIC PERSPECTIVES:\n",
        "   - International historian: Look for references to international relations, treaties, wars, or cross-border interactions that occurred in the past\n",
        "   - Global historian: Consider references to world systems, long-term global trends, or cross-cultural historical developments\n",
        "   - European historian: Note references to European historical periods, events, or figures\n",
        "\n",
        "After your individual analyses, debate any disagreements and reach a consensus.\n",
        "\n",
        "Your final answer must be ONLY 'yes' or 'no' - nothing else.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## define functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def analyze_prompt(prompt, client, retries=3, backoff_factor=2):\n",
        "    \"\"\"Analyze a single prompt using Claude API\"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = client.messages.create(\n",
        "                model=\"claude-3-7-sonnet-20250219\",\n",
        "                max_tokens=10,\n",
        "                temperature=0,\n",
        "                system=system_prompt,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "            \n",
        "            # Extract the yes/no response\n",
        "            classification = response.content[0].text.strip().lower()\n",
        "            # Normalize the response\n",
        "            if \"yes\" in classification:\n",
        "                return \"yes\"\n",
        "            else:\n",
        "                return \"no\"\n",
        "                \n",
        "        except Exception as e:\n",
        "            if attempt < retries - 1:\n",
        "                sleep_time = backoff_factor ** attempt\n",
        "                logger.warning(f\"Error on prompt: {e}. Retrying in {sleep_time}s...\")\n",
        "                time.sleep(sleep_time)\n",
        "            else:\n",
        "                logger.error(f\"Failed after {retries} attempts: {e}\")\n",
        "                return \"error\"\n",
        "\n",
        "def process_batch(batch_prompts, client, batch_id):\n",
        "    \"\"\"Process a batch of prompts\"\"\"\n",
        "    results = []\n",
        "    for prompt in tqdm(batch_prompts, desc=f\"Batch {batch_id}\"):\n",
        "        result = analyze_prompt(prompt, client)\n",
        "        results.append({\"prompt\": prompt, \"references_past\": result})\n",
        "    \n",
        "    # Save batch results\n",
        "    batch_df = pd.DataFrame(results)\n",
        "    batch_df.to_csv(f\"data/results_claude/batch_{batch_id}_results.csv\", index=False)\n",
        "    logger.info(f\"Completed batch {batch_id} with {len(results)} prompts\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load your dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fourth cell - Load and explore dataset\n",
        "\n",
        "# Method 1: load a sample\n",
        "\n",
        "# We want a sample\n",
        "# Define the size of your sample\n",
        "sample_size = 50000\n",
        "\n",
        "# Determine the total number of rows in your dataset\n",
        "total_rows = sum(1 for _ in open('data/prompts.csv')) - 1  # -1 for header\n",
        "\n",
        "# Compute the probability of being selected for each line\n",
        "skip_prob = 1 - sample_size / total_rows\n",
        "\n",
        "# Use the skiprows parameter with a lambda function\n",
        "df = pd.read_csv('data/prompts.csv', \n",
        "                        usecols=[0],  # First column only\n",
        "                        skiprows=lambda x: x > 0 and np.random.random() < skip_prob)\n",
        "\n",
        "# Si l'on veut juste charger tout le fichier -- attention 10 millions de lignes\n",
        "# prompts_df = pd.read_csv('data/prompts.csv', usecols=[0])\n",
        "\n",
        "# on vérifie ce que l'on a engendré:\n",
        "df.shape\n",
        "df.head(10)\n",
        "\n",
        "# Method 2 Your dataset is small, you do not need a sample\n",
        "# df = pd.read_csv(\"data/prompts.csv\")\n",
        "\n",
        "\n",
        "#try:\n",
        "#    df = pd.read_csv(\"data/prompts.csv\", nrows=10000, usecols=[0])\n",
        "#    print(f\"Loaded dataset with {len(df)} prompts\")\n",
        "#   \n",
        "#    # Display a few example prompts\n",
        "#    display(df.head())\n",
        "#    \n",
        "    # If you're working with a sample for testing\n",
        "    # sample_df = df.sample(n=1000)\n",
        "    # prompts_list = sample_df['prompt'].tolist()\n",
        "#    prompts_list = df['prompt'].tolist()\n",
        "#    \n",
        "#except Exception as e:\n",
        "#    print(f\"Error loading dataset: {e}\")\n",
        "    # Create a sample dataset for testing\n",
        "#    prompts_list = [\n",
        "#        \"What will the weather be like tomorrow?\",\n",
        "#        \"Tell me about World War II\",\n",
        "#        \"How do I make a cake?\",\n",
        "#        \"What was the stock market like last year?\",\n",
        "#        \"Remember that time when...\"\n",
        "#    ]\n",
        "#    print(f\"Created sample dataset with {len(prompts_list)} prompts\")\n",
        "#\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process a small test batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fifth cell - Process a small test batch\n",
        "# Test with a small batch first\n",
        "test_batch = prompts_list[:5]  # Just 5 prompts for testing\n",
        "test_results = process_batch(test_batch, client, \"test\")\n",
        "\n",
        "# Display test results\n",
        "test_df = pd.DataFrame(test_results)\n",
        "display(test_df)\n",
        "\n",
        "# Check if everything is working correctly\n",
        "if len(test_df) == len(test_batch):\n",
        "    print(\"✅ Test batch processed successfully!\")\n",
        "else:\n",
        "    print(\"❌ Some issues with the test batch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure and start main processing with optimized parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure and start main processing with optimized parameters\n",
        "# Calculate optimal batch size and workers based on Claude API limits\n",
        "# Rate limits:\n",
        "# - 50 requests per minute\n",
        "# - 20,000 input tokens per minute\n",
        "# - 8,000 output tokens per minute\n",
        "# Pricing:\n",
        "# - Input: $3 per million tokens\n",
        "# - Output: $15 per million tokens\n",
        "\n",
        "# Estimate tokens per prompt (system prompt + user prompt)\n",
        "system_prompt_tokens = 350  # Our complex historian prompt is ~350 tokens\n",
        "avg_prompt_tokens = 50  # Average user prompt length in tokens\n",
        "output_tokens = 5  # Just \"yes\" or \"no\" plus a tiny bit of overhead\n",
        "\n",
        "# Calculate tokens per request\n",
        "tokens_per_request = system_prompt_tokens + avg_prompt_tokens + output_tokens\n",
        "\n",
        "# Calculate how many requests we can make per minute based on different constraints\n",
        "requests_per_min_rate_limit = 50  # Hard limit\n",
        "requests_per_min_input_tokens = 20000 / (system_prompt_tokens + avg_prompt_tokens)\n",
        "requests_per_min_output_tokens = 8000 / output_tokens\n",
        "\n",
        "# The limiting factor is the minimum of these three\n",
        "max_requests_per_min = min(requests_per_min_rate_limit, \n",
        "                           requests_per_min_input_tokens,\n",
        "                           requests_per_min_output_tokens)\n",
        "\n",
        "print(f\"Limiting factors for requests per minute:\")\n",
        "print(f\"- Rate limit: {requests_per_min_rate_limit:.1f} requests/min\")\n",
        "print(f\"- Input token limit: {requests_per_min_input_tokens:.1f} requests/min\")\n",
        "print(f\"- Output token limit: {requests_per_min_output_tokens:.1f} requests/min\")\n",
        "print(f\"→ Maximum throughput: {max_requests_per_min:.1f} requests/min\")\n",
        "\n",
        "# Calculate optimal max_workers (slightly under the limit to allow for overhead)\n",
        "max_workers = max(1, int(max_requests_per_min * 0.9))\n",
        "\n",
        "# Calculate optimal batch_size \n",
        "# For this task, since each prompt is independent, smaller batches are better for parallelism\n",
        "# But we want to save results frequently. A good rule is saving every ~1-2 minutes\n",
        "batch_size = max(5, min(100, int(max_workers * 2)))  # Between 5 and 100 prompts per batch\n",
        "\n",
        "# Split dataset into batches\n",
        "batches = [prompts_list[i:i+batch_size] for i in range(0, len(prompts_list), batch_size)]\n",
        "print(f\"\\nOptimized parameters:\")\n",
        "print(f\"- max_workers: {max_workers} (parallel requests)\")\n",
        "print(f\"- batch_size: {batch_size} prompts per batch\")\n",
        "print(f\"- Total batches: {len(batches)}\")\n",
        "\n",
        "# Check for existing results to resume\n",
        "existing_batches = set()\n",
        "for filename in os.listdir(\"data/results_claude\"):\n",
        "    if filename.startswith(\"batch_\") and filename.endswith(\"_results.csv\"):\n",
        "        try:\n",
        "            batch_id = int(filename.split(\"_\")[1])\n",
        "            existing_batches.add(batch_id)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Get batches to process\n",
        "remaining_batches = [(i, batch) for i, batch in enumerate(batches) if i not in existing_batches]\n",
        "print(f\"\\nResume status: {len(batches)-len(remaining_batches)} batches already completed, {len(remaining_batches)} remaining\")\n",
        "\n",
        "# More accurate cost estimation\n",
        "total_prompts = len(prompts_list)\n",
        "total_input_tokens = total_prompts * (system_prompt_tokens + avg_prompt_tokens)\n",
        "total_output_tokens = total_prompts * output_tokens\n",
        "input_cost = (total_input_tokens / 1_000_000) * 3\n",
        "output_cost = (total_output_tokens / 1_000_000) * 15\n",
        "total_cost = input_cost + output_cost\n",
        "\n",
        "# Calculate time estimate\n",
        "processing_time_minutes = total_prompts / max_requests_per_min\n",
        "processing_time_hours = processing_time_minutes / 60\n",
        "\n",
        "print(f\"\\nEstimated costs:\")\n",
        "print(f\"- Input tokens: {total_input_tokens:,} tokens (${input_cost:.2f})\")\n",
        "print(f\"- Output tokens: {total_output_tokens:,} tokens (${output_cost:.2f})\")\n",
        "print(f\"- Total estimated cost: ${total_cost:.2f}\")\n",
        "print(f\"\\nEstimated processing time: {processing_time_minutes:.1f} minutes ({processing_time_hours:.2f} hours)\")\n",
        "print(f\"Processing {len(remaining_batches)} batches with {batch_size} prompts each using {max_workers} workers\")\n",
        "\n",
        "# Ask for confirmation before proceeding with the full run\n",
        "from IPython.display import display\n",
        "from ipywidgets import widgets\n",
        "\n",
        "confirm = widgets.Button(\n",
        "    description='Start Processing',\n",
        "    button_style='success',\n",
        "    tooltip='Click to start processing all batches'\n",
        ")\n",
        "cancel = widgets.Button(\n",
        "    description='Cancel',\n",
        "    button_style='danger',\n",
        "    tooltip='Cancel processing'\n",
        ")\n",
        "\n",
        "# Add option to adjust parameters\n",
        "worker_slider = widgets.IntSlider(\n",
        "    value=max_workers,\n",
        "    min=1,\n",
        "    max=min(50, max_workers*2),\n",
        "    description='Workers:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "batch_slider = widgets.IntSlider(\n",
        "    value=batch_size,\n",
        "    min=5,\n",
        "    max=200,\n",
        "    description='Batch size:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<b>Adjust processing parameters (if needed):</b>\"),\n",
        "    worker_slider,\n",
        "    batch_slider,\n",
        "    widgets.HBox([confirm, cancel])\n",
        "]), output)\n",
        "\n",
        "def on_confirm(b):\n",
        "    with output:\n",
        "        print(f\"Starting processing with {worker_slider.value} workers and batch size of {batch_slider.value}...\")\n",
        "        # The actual processing code will be in the next cell, and will use these values\n",
        "        \n",
        "def on_cancel(b):\n",
        "    with output:\n",
        "        print(\"Processing cancelled.\")\n",
        "\n",
        "confirm.on_click(on_confirm)\n",
        "cancel.on_click(on_cancel)\n",
        "\n",
        "# Store these values to be used in the next cell\n",
        "processing_config = {\n",
        "    'max_workers': worker_slider,\n",
        "    'batch_size': batch_slider\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute processing (run after confirmation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute processing (run after confirmation)\n",
        "# Process batches with multiple threads\n",
        "all_results = []\n",
        "\n",
        "# Use tqdm to track overall progress\n",
        "progress_bar = tqdm(total=len(remaining_batches), desc=\"Overall progress\")\n",
        "\n",
        "# Function to update progress\n",
        "def update_progress(future):\n",
        "    progress_bar.update(1)\n",
        "    return future.result()\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "    futures = []\n",
        "    for i, batch in remaining_batches:\n",
        "        future = executor.submit(process_batch, batch, client, i)\n",
        "        future.add_done_callback(update_progress)\n",
        "        futures.append(future)\n",
        "    \n",
        "    # Wait for all futures to complete\n",
        "    for future in futures:\n",
        "        try:\n",
        "            batch_results = future.result()\n",
        "            all_results.extend(batch_results)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing batch: {e}\")\n",
        "\n",
        "progress_bar.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combine results and generate report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine results and generate report\n",
        "# Read all batch results\n",
        "all_batches = []\n",
        "for filename in os.listdir(\"results\"):\n",
        "    if filename.startswith(\"batch_\") and filename.endswith(\"_results.csv\"):\n",
        "        try:\n",
        "            batch_df = pd.read_csv(f\"results/{filename}\")\n",
        "            all_batches.append(batch_df)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error reading {filename}: {e}\")\n",
        "\n",
        "# Combine all results\n",
        "if all_batches:\n",
        "    final_df = pd.concat(all_batches, ignore_index=True)\n",
        "    final_df.to_csv(\"results/past_references_complete_results.csv\", index=False)\n",
        "    print(f\"Combined results from {len(all_batches)} batches, total {len(final_df)} prompts\")\n",
        "    \n",
        "    # Generate summary statistics\n",
        "    past_references_count = sum(final_df['references_past'] == 'yes')\n",
        "    error_count = sum(final_df['references_past'] == 'error')\n",
        "    \n",
        "    print(\"\\n--- SUMMARY STATISTICS ---\")\n",
        "    print(f\"Total prompts processed: {len(final_df)}\")\n",
        "    print(f\"Prompts with past references: {past_references_count} ({past_references_count/len(final_df)*100:.2f}%)\")\n",
        "    print(f\"Prompts with errors: {error_count} ({error_count/len(final_df)*100:.2f}%)\")\n",
        "    \n",
        "    # Display a sample of prompts with past references\n",
        "    print(\"\\n--- SAMPLE PROMPTS WITH PAST REFERENCES ---\")\n",
        "    past_refs = final_df[final_df['references_past'] == 'yes'].sample(min(5, past_references_count))\n",
        "    display(past_refs)\n",
        "else:\n",
        "    print(\"No batch results found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization and analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization and analysis\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set a nicer style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a pie chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "counts = final_df['references_past'].value_counts()\n",
        "plt.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=90, colors=sns.color_palette(\"Set2\"))\n",
        "plt.axis('equal')\n",
        "plt.title('Distribution of Prompts with Past References')\n",
        "plt.show()\n",
        "\n",
        "# Optionally, if your dataset has other columns (e.g., prompt length, category, etc.)\n",
        "# You can do more analysis to look for patterns\n",
        "\n",
        "# Add a prompt length column\n",
        "final_df['prompt_length'] = final_df['prompt'].str.len()\n",
        "\n",
        "# Plot prompt length distribution by reference type\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(data=final_df, x='prompt_length', hue='references_past', multiple='stack', bins=30)\n",
        "plt.title('Prompt Length Distribution by Reference Type')\n",
        "plt.xlabel('Prompt Length (characters)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Any additional analysis you might want to perform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "citation-manager": {
      "items": {}
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "negotiating_past",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
