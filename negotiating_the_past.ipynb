{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncks0uTDzzbr"
      },
      "source": [
        "# negotiating the past"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAMm3ta9zzbt"
      },
      "source": [
        "To install the environment, please read README.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9P3w6zjzzbu"
      },
      "source": [
        "## Project Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOnXyO8-zzbv"
      },
      "source": [
        "This presentation explores the intersection of historical imagination, artificial intelligence, and collective memory. We'll examine how users \"negotiate\" with AI systems to express their conceptions of the past, and how these interactions can reveal tensions between user expectations and AI-embedded historical patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3hVHSpLzzbv"
      },
      "source": [
        "## Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gYah1yFzzbw"
      },
      "source": [
        "1. **Theoretical Framework**: How LLMs encode historical perspectives\n",
        "2. **Methodological Approach**: Analyzing historical references in prompts\n",
        "3. **Results Analysis**: What prompt analysis reveals about historical imagination\n",
        "4. **Conclusion**: New spaces for historical negotiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u9weciKzzbw"
      },
      "source": [
        "# Part I: Theoretical Framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hyk3aN1Wzzbx"
      },
      "source": [
        "## LLMs and Embedded Historical Patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmzyS9Vqzzbx"
      },
      "source": [
        "- LLMs as Statistical Pattern Recognizers\n",
        "- Training Data as Historical Record\n",
        "- Historical Biases in Language Models\n",
        "- \"Stochastic Parrots\" and Historical Truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InBd6u7Vzzbz"
      },
      "source": [
        "## Technical Foundation of LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OpZ0_Mtzzbz"
      },
      "source": [
        "LLMs rely on transformer architectures that predict tokens based on previous context. Their \"knowledge\" of history comes from statistical patterns in training data, not genuine understanding. This creates an interesting dynamic when users prompt these systems about historical topics - the system's responses reveal embedded historical narratives from their training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2oCbKbtzzbz"
      },
      "source": [
        "## Historical Knowledge in Vector Space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPItExUhzzb0"
      },
      "source": [
        "- Word embeddings capture semantic relationships\n",
        "- Historical concepts represented as vectors\n",
        "- Temporal relationships encoded in semantic proximity\n",
        "- Cultural associations embedded in language patterns\n",
        "\n",
        "Within the vector space of LLMs, historical concepts are encoded as points in multidimensional space. The relationships between historical events, figures, and concepts are captured in the distances and directions between these vectors. These semantic relationships reflect collective memory patterns from the training corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90GoVLlIzzb0"
      },
      "source": [
        "## Collective Memory and LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77zYNsdbzzb0"
      },
      "source": [
        "- LLMs as repositories of digitized collective memory\n",
        "- Training data selection as memory politics\n",
        "- The \"averaged\" nature of AI-generated historical narratives\n",
        "- Absence of contested memory in statistical consensus\n",
        "\n",
        "From a memory studies perspective, LLMs function as repositories of digitized collective memory. The selection of training data constitutes a form of memory politics, determining which historical perspectives are included or excluded. The statistical nature of these models produces \"averaged\" historical narratives that often elide contestation and complexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDuaGCSnzzb0"
      },
      "source": [
        "# Part II: Methodological Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmRnFRyxzzb1"
      },
      "source": [
        "## The Challenge of Historical Prompt Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgpMAiaYzzb1"
      },
      "source": [
        "- Beyond simple keyword approaches\n",
        "- Historical references: explicit vs. implicit\n",
        "- Temporality in language\n",
        "- Building a robust identification strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHjGRP_Xzzb1"
      },
      "source": [
        "Identifying prompts that reference history requires more sophisticated approaches than simple keyword matching. Historical references can be explicit (\"Napoleon Bonaparte\") or implicit (\"the Emperor's exile\"), and may involve complex temporal markers. Our methodology must capture this complexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PrCaiB6xzzb1",
        "outputId": "fba329cc-d65e-4b2c-b6e3-9e8e1de65666",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.12\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (0.5.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (1.6.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from umap-learn) (4.67.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->umap-learn) (3.6.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!pip install umap-learn\n",
        "!pip install gensim\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vtFna26EPTe",
        "outputId": "3d97361f-8aee-4355-880b-f70910bbd1d5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaKgggqHzzb2"
      },
      "source": [
        "## Creating a Historical Prompt Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dX5tD0evzzb3",
        "outputId": "2fe8b9c7-e222-4e44-c2b8-0e35da82c2ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Loading our dataset of 10 million prompts\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import ssl\n",
        "\n",
        "# Make sure you have the necessary NLTK resources\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Read the CSV file\n",
        "prompts_df = pd.read_csv(\"drive/MyDrive/data/prompts.csv\", on_bad_lines='skip')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWV0Mqlazzb3"
      },
      "source": [
        "## Historical Reference Detection Approach With DistilBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i7kCiBnizzb3",
        "outputId": "49dde92d-e787-422b-8bee-659fd0066d10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399,
          "referenced_widgets": [
            "62c3170c2f24407abb87bc93afa5f063",
            "49068d3445b14602b76c14c92e50a409",
            "a66c63f3ca934e99be65840c0f5b90e7",
            "e043f8479bf1454885c3b317fe946142",
            "7cc4f9a7ecdc462c920b32fd7d2ea5a7",
            "2cb25dd06f45466bb960803f4dbb2840",
            "8040b1cc982c45e28e83bbaf03451319",
            "c6222c98afbb44de881f2902f45f2247",
            "be2ccb548a9d4e518694c4e588388eb3",
            "343ac99d91cd4adba46bdcc7b68766a0",
            "6e2272babb314360a6cb04ed203e782c",
            "d71516e36efb42859213dd8ef59e9884",
            "e4438105186b4733b5cf246db38e0c61",
            "4f6d17d4bb8d44bfb652e1da461885a3",
            "56e0845543d64ff0832a85535fc5a035",
            "741afd0679a647288d8ba753514d3618",
            "ade41a469015466cb71427fadc712217",
            "3c70c76956114640849a009014304979",
            "02e187d3ad624bf5815a39592e6f95a3",
            "8c8cebba2d5448e885a4e3206158341e",
            "eece8b7ac86242aebfb9cffefa14b6d5",
            "84df3ae560c44a8d92f2c98c43a4e0bc",
            "4eddb4280ea94723b1568de69b3d96f4",
            "9a487b7462ba4192b1ce4a327e91ad7a",
            "22cd6800c2c64cb8b77d96d6ab01649e",
            "89db48cc382749ee8f8ccb2c8a39e361",
            "35bbe9ce17324b77b46de5a53ea3e51f",
            "ff3274e1c9e6415aab3e7a35d3a46c6b",
            "7de7bbdcc624418a8ac714e55635caec",
            "5d1d198cf4f0464ab4a55f3570404053",
            "372ee47421d24971bddc0f5d47df6e28",
            "9e19d9ea7ed14c56a7bdc58aa06b6b31",
            "c9ef3b3addda4bb6ae8adbaa4f6912ce",
            "9cdfa8d8314a484984c31d7494fd6052",
            "4482c1c111cd483c999d4c224ea0dd22",
            "5e16b04e795b4db99c9458349e71f3f2",
            "16d2daa80e3f45d6aa17821bd94b6634",
            "5b3e0a4a19ea4681b1edf563d704364d",
            "454e8640ef244b619f0fcfe7a092e000",
            "52c7322437314574aa2ea75d5ba11cae",
            "85dd4352a94d4913a74ec9821263c3b3",
            "04faf007a9cd49c7bb4e424f182934e9",
            "9643879863a441cdb04fb47d707edbda",
            "fc56808171464c25906f36c9c1efa45a",
            "21f4c2f3b0834df980cbbdf6ed72f098",
            "d64e53ba5ed642c9b134349e3ca77b06",
            "32a7d9b907b5468fb6857bd10ce6d21e",
            "45ccf3b24509415d8ed5f1126bb2610a",
            "69ffd53a959140dca6051746fa01c491",
            "5572b8459c5c459b8f32ef2f28c99cd0",
            "74db9695956144aa96c90b364868125b",
            "6fbd871893974b19a97493a5c97ede04",
            "ddeb11a5efc44ae299a4f245f96aeb9f",
            "a70411d94b594869be10c1c110e82c52",
            "9e6be02d85114d938d415011d7b8bcf5"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading DistilBERT model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62c3170c2f24407abb87bc93afa5f063"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d71516e36efb42859213dd8ef59e9884"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4eddb4280ea94723b1568de69b3d96f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9cdfa8d8314a484984c31d7494fd6052"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21f4c2f3b0834df980cbbdf6ed72f098"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on CPU\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "from umap import UMAP\n",
        "from sklearn.cluster import DBSCAN\n",
        "from tqdm import tqdm  # For progress bars\n",
        "import gc  # Garbage collection\n",
        "import multiprocessing\n",
        "from functools import partial\n",
        "import os\n",
        "\n",
        "# Set the number of CPU cores to use\n",
        "num_cpu_cores = 8\n",
        "torch.set_num_threads(num_cpu_cores)  # Set PyTorch to use your 8 cores\n",
        "os.environ[\"OMP_NUM_THREADS\"] = str(num_cpu_cores)  # OpenMP threads\n",
        "os.environ[\"MKL_NUM_THREADS\"] = str(num_cpu_cores)  # MKL threads\n",
        "\n",
        "# Load DistilBERT model and tokenizer\n",
        "print(\"Loading DistilBERT model...\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Switch to CPU mode explicitly\n",
        "model = model.to(\"cpu\")\n",
        "print(\"Model loaded on CPU\")\n",
        "\n",
        "# Function to get DistilBERT embeddings for a batch of texts\n",
        "def process_batch(batch_texts, tokenizer, model):\n",
        "    # Filter out None values and empty strings\n",
        "    batch_texts = [text for text in batch_texts if isinstance(text, str) and text.strip()]\n",
        "\n",
        "    if not batch_texts:\n",
        "        return np.array([])\n",
        "\n",
        "    # Tokenize the texts\n",
        "    encoded_input = tokenizer(batch_texts, padding=True, truncation=True,\n",
        "                             max_length=256, return_tensors='pt')\n",
        "\n",
        "    # Compute token embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded_input)\n",
        "\n",
        "    # Use the [CLS] token embedding as the sentence embedding\n",
        "    sentence_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "\n",
        "    # Clear CUDA cache if using GPU\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return sentence_embeddings\n",
        "\n",
        "# Function to split a list into chunks\n",
        "def chunks(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "# Function to get embeddings with multiprocessing\n",
        "def get_distilbert_embeddings_parallel(texts, tokenizer, model, batch_size=32, n_processes=8):\n",
        "    # Split texts into batches\n",
        "    batches = list(chunks(texts, batch_size))\n",
        "\n",
        "    # Process batches in parallel\n",
        "    with multiprocessing.Pool(processes=n_processes) as pool:\n",
        "        process_func = partial(process_batch, tokenizer=tokenizer, model=model)\n",
        "        results = list(tqdm(pool.imap(process_func, batches), total=len(batches), desc=\"Generating embeddings\"))\n",
        "\n",
        "    # Filter out empty results and concatenate\n",
        "    results = [r for r in results if r.size > 0]\n",
        "    if not results:\n",
        "        return np.array([])\n",
        "\n",
        "    return np.concatenate(results, axis=0)\n",
        "\n",
        "# Memory-efficient function to calculate cosine similarities in chunks\n",
        "def calculate_similarities_in_chunks(embeddings_a, embeddings_b, chunk_size=1000):\n",
        "    n_samples = embeddings_a.shape[0]\n",
        "    n_features = embeddings_b.shape[0]\n",
        "    similarities = np.zeros((n_samples, n_features))\n",
        "\n",
        "    for i in tqdm(range(0, n_samples, chunk_size), desc=\"Calculating similarities\"):\n",
        "        end_idx = min(i + chunk_size, n_samples)\n",
        "        chunk_similarities = cosine_similarity(\n",
        "            embeddings_a[i:end_idx],\n",
        "            embeddings_b\n",
        "        )\n",
        "        similarities[i:end_idx] = chunk_similarities\n",
        "\n",
        "    return similarities\n",
        "\n",
        "# Main processing pipeline\n",
        "def process_prompts_for_historical_content(prompts_df, sample_size=100000):\n",
        "    # Sample if dataset is large\n",
        "    if len(prompts_df) > sample_size:\n",
        "        sample_df = prompts_df.sample(sample_size, random_state=42)\n",
        "        print(f\"Sampled {sample_size} prompts from {len(prompts_df)} total\")\n",
        "    else:\n",
        "        sample_df = prompts_df.copy()\n",
        "        print(f\"Processing all {len(sample_df)} prompts\")\n",
        "\n",
        "    # Clean memory\n",
        "    gc.collect()\n",
        "\n",
        "    # Get list of prompts\n",
        "    prompts_list = sample_df['prompt'].fillna('').tolist()\n",
        "\n",
        "    # Generate embeddings in parallel\n",
        "    print(\"Generating prompt embeddings...\")\n",
        "    prompt_embeddings = get_distilbert_embeddings_parallel(\n",
        "        prompts_list,\n",
        "        tokenizer,\n",
        "        model,\n",
        "        batch_size=32,  # Smaller batches for memory efficiency\n",
        "        n_processes=num_cpu_cores\n",
        "    )\n",
        "\n",
        "    # Define historical concepts\n",
        "    historical_concepts = [\n",
        "        \"ancient history\", \"medieval times\", \"renaissance period\",\n",
        "        \"world war\", \"cold war\", \"industrial revolution\",\n",
        "        \"historic events\", \"historical figures\", \"archaeological findings\",\n",
        "        \"prehistoric era\", \"colonial period\", \"civil rights movement\",\n",
        "        \"past civilizations\", \"historical artifacts\", \"ancient empires\",\n",
        "        \"historical battles\", \"kings and queens\", \"historical architecture\"\n",
        "    ]\n",
        "\n",
        "    # Generate embeddings for historical concepts\n",
        "    print(\"Generating historical concept embeddings...\")\n",
        "    historical_embeddings = get_distilbert_embeddings_parallel(\n",
        "        historical_concepts,\n",
        "        tokenizer,\n",
        "        model,\n",
        "        batch_size=len(historical_concepts),  # Process all concepts in one batch\n",
        "        n_processes=1  # No need for parallelism with small number of concepts\n",
        "    )\n",
        "\n",
        "    # Clean memory\n",
        "    gc.collect()\n",
        "\n",
        "    # Calculate similarities in memory-efficient chunks\n",
        "    print(\"Calculating similarities...\")\n",
        "    similarity_matrix = calculate_similarities_in_chunks(\n",
        "        prompt_embeddings,\n",
        "        historical_embeddings,\n",
        "        chunk_size=1000\n",
        "    )\n",
        "\n",
        "    # For each prompt, get the maximum similarity to any historical concept\n",
        "    max_similarities = np.max(similarity_matrix, axis=1)\n",
        "\n",
        "    # Find the index of the most similar historical concept for each prompt\n",
        "    most_similar_concept_idx = np.argmax(similarity_matrix, axis=1)\n",
        "    sample_df['most_similar_concept'] = [historical_concepts[idx] for idx in most_similar_concept_idx]\n",
        "    sample_df['similarity_score'] = max_similarities\n",
        "\n",
        "    # Filter prompts with similarity scores above a threshold\n",
        "    threshold = 0.6  # Adjust based on your needs\n",
        "    historical_prompts = sample_df[max_similarities > threshold].copy()\n",
        "\n",
        "    print(f\"Identified {len(historical_prompts)} historical prompts ({len(historical_prompts)/len(sample_df):.2%} of sample)\")\n",
        "\n",
        "    # Clean memory before UMAP/clustering\n",
        "    del similarity_matrix\n",
        "    gc.collect()\n",
        "\n",
        "    return historical_prompts, prompt_embeddings, max_similarities\n",
        "\n",
        "# Assuming prompts_df is already loaded\n",
        "# historical_prompts, prompt_embeddings, max_similarities = process_prompts_for_historical_content(prompts_df)\n",
        "\n",
        "# The rest of the code (UMAP and clustering) can remain largely the same as before\n",
        "def analyze_historical_clusters(historical_prompts, prompt_embeddings, max_similarities, threshold=0.6):\n",
        "    if len(historical_prompts) > 10:  # Need at least a few samples for clustering\n",
        "        # Get embeddings for the filtered historical prompts\n",
        "        historical_embeddings = prompt_embeddings[max_similarities > threshold]\n",
        "\n",
        "        print(\"Applying UMAP dimensionality reduction...\")\n",
        "        # Apply UMAP with CPU optimization\n",
        "        umap_model = UMAP(n_neighbors=15, min_dist=0.1, random_state=42,\n",
        "                          n_jobs=num_cpu_cores)  # Use all cores\n",
        "        historical_umap = umap_model.fit_transform(historical_embeddings)\n",
        "\n",
        "        print(\"Clustering with DBSCAN...\")\n",
        "        # Apply DBSCAN\n",
        "        dbscan = DBSCAN(eps=0.5, min_samples=5, n_jobs=num_cpu_cores)  # Use all cores\n",
        "        cluster_labels = dbscan.fit_predict(historical_umap)\n",
        "\n",
        "        # Add cluster labels to the dataframe\n",
        "        historical_prompts['cluster'] = cluster_labels\n",
        "\n",
        "        # Generate visualizations and report\n",
        "        generate_cluster_report(historical_prompts, historical_umap, cluster_labels)\n",
        "\n",
        "        return historical_prompts, historical_umap, cluster_labels\n",
        "    else:\n",
        "        print(\"Not enough historical prompts for clustering\")\n",
        "        return historical_prompts, None, None\n",
        "\n",
        "def generate_cluster_report(historical_prompts, historical_umap, cluster_labels):\n",
        "    # Count number of prompts per cluster\n",
        "    cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
        "    print(\"\\nNumber of prompts per cluster:\")\n",
        "    print(cluster_counts)\n",
        "\n",
        "    # Visualize the clusters\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    # Exclude noise points (cluster -1) for better visualization\n",
        "    non_noise_mask = cluster_labels != -1\n",
        "\n",
        "    scatter = plt.scatter(\n",
        "        historical_umap[non_noise_mask, 0],\n",
        "        historical_umap[non_noise_mask, 1],\n",
        "        c=cluster_labels[non_noise_mask],\n",
        "        cmap='tab20',\n",
        "        alpha=0.6,\n",
        "        s=10\n",
        "    )\n",
        "    plt.colorbar(scatter, label='Cluster')\n",
        "    plt.title('Clusters of Historical Prompts (DistilBERT method)')\n",
        "    plt.xlabel('UMAP Dimension 1')\n",
        "    plt.ylabel('UMAP Dimension 2')\n",
        "    plt.savefig('historical_prompt_clusters_distilbert.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()  # Close the figure to free memory\n",
        "\n",
        "    # Sample prompts from each cluster\n",
        "    print(\"\\nSample prompts from each cluster:\")\n",
        "    for cluster_id in sorted(historical_prompts['cluster'].unique()):\n",
        "        if cluster_id == -1:\n",
        "            continue  # Skip noise points\n",
        "\n",
        "        cluster_prompts = historical_prompts[historical_prompts['cluster'] == cluster_id]\n",
        "        print(f\"\\nCluster {cluster_id} ({len(cluster_prompts)} prompts):\")\n",
        "\n",
        "        # Sample up to 5 prompts from this cluster\n",
        "        samples = cluster_prompts.sample(min(5, len(cluster_prompts)))\n",
        "        for _, row in samples.iterrows():\n",
        "            print(f\"  - {row['prompt'][:100]}... (Similarity: {row['similarity_score']:.2f}, Concept: {row['most_similar_concept']})\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your data\n",
        "    # prompts_df = pd.read_csv(\"prompts.csv\")\n",
        "\n",
        "    # Run the complete pipeline\n",
        "    # historical_prompts, prompt_embeddings, max_similarities = process_prompts_for_historical_content(prompts_df)\n",
        "    # historical_prompts, umap_result, cluster_labels = analyze_historical_clusters(\n",
        "    #     historical_prompts, prompt_embeddings, max_similarities)\n",
        "\n",
        "    print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prompts_df = pd.read_csv(\"prompts.csv\")\n",
        "\n",
        "# Run the complete pipeline\n",
        "historical_prompts, prompt_embeddings, max_similarities = process_prompts_for_historical_content(prompts_df)\n",
        "historical_prompts, umap_result, cluster_labels = analyze_historical_clusters(\n",
        "  historical_prompts, prompt_embeddings, max_similarities)"
      ],
      "metadata": {
        "id": "YkAkm77RHuQq",
        "outputId": "f099309c-7d31-4770-c0fc-99c64833a53b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing all 99852 prompts\n",
            "Generating prompt embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating embeddings:  29%|██▉       | 899/3121 [1:19:17<1:48:33,  2.93s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpR84gfZzzb4"
      },
      "source": [
        "### 1. Visualisation de la distribution des scores de similarité\n",
        "\n",
        "Cette cellule permet de visualiser comment les scores de similarité sont distribués et d'évaluer si le seuil choisi (0.6) est approprié"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "84EXA6Vuzzb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "89d85873-90d1-46ba-c94e-25f44c8e9afe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing all 99852 prompts\n",
            "Generating prompt embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating embeddings:   0%|          | 0/3121 [00:42<?, ?it/s]Process ForkPoolWorker-8:\n",
            "\n",
            "Process ForkPoolWorker-2:\n",
            "Process ForkPoolWorker-6:\n",
            "Process ForkPoolWorker-7:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "                    ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "                    ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "                    ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-2-fcd2399497a8>\", line 44, in process_batch\n",
            "    outputs = model(**encoded_input)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-2-fcd2399497a8>\", line 39, in process_batch\n",
            "    encoded_input = tokenizer(batch_texts, padding=True, truncation=True,\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "                    ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-2-fcd2399497a8>\", line 44, in process_batch\n",
            "    outputs = model(**encoded_input)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2887, in __call__\n",
            "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "Process ForkPoolWorker-4:\n",
            "  File \"<ipython-input-2-fcd2399497a8>\", line 39, in process_batch\n",
            "    encoded_input = tokenizer(batch_texts, padding=True, truncation=True,\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2975, in _call_one\n",
            "    return self.batch_encode_plus(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2887, in __call__\n",
            "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 3177, in batch_encode_plus\n",
            "    return self._batch_encode_plus(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 796, in forward\n",
            "    return self.transformer(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils.py\", line 887, in _batch_encode_plus\n",
            "    first_ids = get_input_ids(ids)\n",
            "                ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils.py\", line 854, in get_input_ids\n",
            "    tokens = self.tokenize(text, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2975, in _call_one\n",
            "    return self.batch_encode_plus(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 796, in forward\n",
            "    return self.transformer(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 3177, in batch_encode_plus\n",
            "    return self._batch_encode_plus(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils.py\", line 697, in tokenize\n",
            "    tokenized_text.extend(self._tokenize(token))\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 549, in forward\n",
            "    layer_outputs = layer_module(\n",
            "                    ^^^^^^^^^^^^^\n",
            "Process ForkPoolWorker-1:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils.py\", line 887, in _batch_encode_plus\n",
            "    first_ids = get_input_ids(ids)\n",
            "                ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/tokenization_distilbert.py\", line 167, in _tokenize\n",
            "    for token in self.basic_tokenizer.tokenize(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 549, in forward\n",
            "    layer_outputs = layer_module(\n",
            "                    ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils.py\", line 854, in get_input_ids\n",
            "    tokens = self.tokenize(text, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/tokenization_distilbert.py\", line 375, in tokenize\n",
            "    split_tokens.extend(self._run_split_on_punc(token, never_split))\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils.py\", line 697, in tokenize\n",
            "    tokenized_text.extend(self._tokenize(token))\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "                    ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/tokenization_distilbert.py\", line 401, in _run_split_on_punc\n",
            "    if _is_punctuation(char):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/tokenization_distilbert.py\", line 167, in _tokenize\n",
            "    for token in self.basic_tokenizer.tokenize(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-2-fcd2399497a8>\", line 44, in process_batch\n",
            "    outputs = model(**encoded_input)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/tokenization_distilbert.py\", line 362, in tokenize\n",
            "    text = self._tokenize_chinese_chars(text)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils.py\", line 375, in _is_punctuation\n",
            "    cat = unicodedata.category(char)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 475, in forward\n",
            "    sa_output = self.attention(\n",
            "                ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/tokenization_distilbert.py\", line 423, in _tokenize_chinese_chars\n",
            "    output.append(char)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 796, in forward\n",
            "    return self.transformer(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 411, in forward\n",
            "    attn_output = self.out_lin(attn_output)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 493, in forward\n",
            "    ffn_output = self.ffn(sa_output)  # (bs, seq_length, dim)\n",
            "                 ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "Traceback (most recent call last):\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "Process ForkPoolWorker-5:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 549, in forward\n",
            "    layer_outputs = layer_module(\n",
            "                    ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 427, in forward\n",
            "    return apply_chunking_to_forward(self.ff_chunk, self.chunk_size_feed_forward, self.seq_len_dim, input)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py\", line 253, in apply_chunking_to_forward\n",
            "    return forward_fn(*input_tensors)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "                    ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 432, in ff_chunk\n",
            "    x = self.lin2(x)\n",
            "        ^^^^^^^^^\n",
            "  File \"<ipython-input-2-fcd2399497a8>\", line 44, in process_batch\n",
            "    outputs = model(**encoded_input)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 475, in forward\n",
            "    sa_output = self.attention(\n",
            "                ^^^^^^^^^^^^^^^\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1915, in __getattr__\n",
            "    def __getattr__(self, name: str) -> Union[Tensor, \"Module\"]:\n",
            "    \n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "                    ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 796, in forward\n",
            "    return self.transformer(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-2-fcd2399497a8>\", line 44, in process_batch\n",
            "    outputs = model(**encoded_input)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 390, in forward\n",
            "    k = shape(self.k_lin(key))  # (bs, n_heads, k_length, dim_per_head)\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 796, in forward\n",
            "    return self.transformer(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 549, in forward\n",
            "    layer_outputs = layer_module(\n",
            "                    ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 549, in forward\n",
            "    layer_outputs = layer_module(\n",
            "                    ^^^^^^^^^^^^^\n",
            "Process ForkPoolWorker-3:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 493, in forward\n",
            "    ffn_output = self.ffn(sa_output)  # (bs, seq_length, dim)\n",
            "                 ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 493, in forward\n",
            "    ffn_output = self.ffn(sa_output)  # (bs, seq_length, dim)\n",
            "                 ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 427, in forward\n",
            "    return apply_chunking_to_forward(self.ff_chunk, self.chunk_size_feed_forward, self.seq_len_dim, input)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py\", line 253, in apply_chunking_to_forward\n",
            "    return forward_fn(*input_tensors)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 427, in forward\n",
            "    return apply_chunking_to_forward(self.ff_chunk, self.chunk_size_feed_forward, self.seq_len_dim, input)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 430, in ff_chunk\n",
            "    x = self.lin1(input)\n",
            "        ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py\", line 253, in apply_chunking_to_forward\n",
            "    return forward_fn(*input_tensors)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 430, in ff_chunk\n",
            "    x = self.lin1(input)\n",
            "        ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "                    ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-2-fcd2399497a8>\", line 44, in process_batch\n",
            "    outputs = model(**encoded_input)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 796, in forward\n",
            "    return self.transformer(\n",
            "           ^^^^^^^^^^^^^^^^^\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9855c931c137>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# D'abord, exécutez le pipeline principal pour générer les données nécessaires\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mhistorical_prompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_similarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_prompts_for_historical_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Exemple d'utilisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-fcd2399497a8>\u001b[0m in \u001b[0;36mprocess_prompts_for_historical_content\u001b[0;34m(prompts_df, sample_size)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Generate embeddings in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating prompt embeddings...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     prompt_embeddings = get_distilbert_embeddings_parallel(\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mprompts_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-fcd2399497a8>\u001b[0m in \u001b[0;36mget_distilbert_embeddings_parallel\u001b[0;34m(texts, tokenizer, model, batch_size, n_processes)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_processes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mprocess_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Generating embeddings\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# Filter out empty results and concatenate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def visualize_similarity_distribution(similarity_scores, threshold=0.6, output_path='./figures/'):\n",
        "    \"\"\"\n",
        "    Visualise la distribution des scores de similarité et marque le seuil utilisé.\n",
        "\n",
        "    Args:\n",
        "        similarity_scores: Tableau numpy des scores de similarité\n",
        "        threshold: Seuil utilisé pour filtrer les prompts historiques\n",
        "        output_path: Chemin pour sauvegarder les figures\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import os\n",
        "\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Histogramme des scores de similarité\n",
        "    plt.hist(similarity_scores, bins=50, alpha=0.7, color='steelblue')\n",
        "\n",
        "    # Ligne verticale pour le seuil\n",
        "    plt.axvline(x=threshold, color='red', linestyle='--',\n",
        "                label=f'Seuil ({threshold})')\n",
        "\n",
        "    # Annotations\n",
        "    plt.title('Distribution des scores de similarité avec les concepts historiques',\n",
        "              fontsize=14)\n",
        "    plt.xlabel('Score de similarité', fontsize=12)\n",
        "    plt.ylabel('Nombre de prompts', fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    # Annotation des statistiques clés\n",
        "    plt.text(0.02, 0.95,\n",
        "             f\"Total: {len(similarity_scores)}\\nHistoriques: {np.sum(similarity_scores > threshold)} ({np.mean(similarity_scores > threshold):.1%})\",\n",
        "             transform=plt.gca().transAxes,\n",
        "             bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_path, 'similarity_distribution.png'), dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# D'abord, exécutez le pipeline principal pour générer les données nécessaires\n",
        "historical_prompts, prompt_embeddings, max_similarities = process_prompts_for_historical_content(prompts_df)\n",
        "\n",
        "# Exemple d'utilisation\n",
        "visualize_similarity_distribution(max_similarities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J781NNphzzb5"
      },
      "source": [
        "### 2. Carte de chaleur des similarités entre concepts historiques\n",
        "\n",
        "Cette cellule permet de visualiser comment les concepts historiques sont reliés entre eux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0YCcMfazzb6"
      },
      "outputs": [],
      "source": [
        "def visualize_historical_concepts_similarity(historical_concepts, historical_embeddings, output_path='./figures/'):\n",
        "    \"\"\"\n",
        "    Crée une carte de chaleur montrant les similarités entre concepts historiques.\n",
        "\n",
        "    Args:\n",
        "        historical_concepts: Liste des concepts historiques\n",
        "        historical_embeddings: Embeddings des concepts historiques\n",
        "        output_path: Chemin pour sauvegarder les figures\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import seaborn as sns\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    import os\n",
        "\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    # Calculer la matrice de similarité entre concepts\n",
        "    concept_similarity = cosine_similarity(historical_embeddings)\n",
        "\n",
        "    # Créer une figure de taille appropriée\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Créer la heatmap avec seaborn\n",
        "    sns.heatmap(concept_similarity, annot=True, fmt=\".2f\", cmap=\"YlGnBu\",\n",
        "                xticklabels=historical_concepts, yticklabels=historical_concepts)\n",
        "\n",
        "    plt.title(\"Similarité entre les concepts historiques\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_path, 'historical_concepts_similarity.png'), dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# Exemple d'utilisation\n",
        "visualize_historical_concepts_similarity(historical_concepts, historical_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHMtSy0yzzb6"
      },
      "source": [
        "### 3. Projection UMAP des prompts avec coloration par concept le plus similaire\n",
        "Cette visualisation permet de voir comment les prompts se regroupent naturellement et si les concepts les plus similaires forment des clusters cohérents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlMNhI9Nzzb6"
      },
      "outputs": [],
      "source": [
        "def visualize_umap_by_concept(historical_prompts, historical_umap, output_path='./figures/'):\n",
        "    \"\"\"\n",
        "    Visualise la projection UMAP des prompts, colorés par concept historique le plus similaire.\n",
        "\n",
        "    Args:\n",
        "        historical_prompts: DataFrame contenant les prompts historiques avec leur concept le plus similaire\n",
        "        historical_umap: Coordonnées UMAP des prompts historiques\n",
        "        output_path: Chemin pour sauvegarder les figures\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import os\n",
        "    from matplotlib.colors import ListedColormap\n",
        "\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    # Obtenir les concepts uniques\n",
        "    unique_concepts = historical_prompts['most_similar_concept'].unique()\n",
        "    n_concepts = len(unique_concepts)\n",
        "\n",
        "    # Créer un mapping des concepts aux indices\n",
        "    concept_to_idx = {concept: i for i, concept in enumerate(unique_concepts)}\n",
        "\n",
        "    # Obtenir un tableau numpy des indices de concepts\n",
        "    concept_indices = np.array([concept_to_idx[concept] for concept in historical_prompts['most_similar_concept']])\n",
        "\n",
        "    # Créer une colormap avec suffisamment de couleurs distinctes\n",
        "    import matplotlib.cm as cm\n",
        "    if n_concepts <= 10:\n",
        "        cmap = ListedColormap(plt.cm.tab10.colors[:n_concepts])\n",
        "    elif n_concepts <= 20:\n",
        "        cmap = ListedColormap(plt.cm.tab20.colors[:n_concepts])\n",
        "    else:\n",
        "        cmap = plt.cm.nipy_spectral\n",
        "\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # Scatter plot avec coloration par concept\n",
        "    scatter = plt.scatter(historical_umap[:, 0], historical_umap[:, 1],\n",
        "                         c=concept_indices, cmap=cmap,\n",
        "                         alpha=0.7, s=10)\n",
        "\n",
        "    # Créer une légende explicite\n",
        "    from matplotlib.lines import Line2D\n",
        "    legend_elements = [Line2D([0], [0], marker='o', color='w',\n",
        "                              markerfacecolor=cmap(concept_to_idx[concept]),\n",
        "                              markersize=8, label=concept)\n",
        "                       for concept in unique_concepts]\n",
        "\n",
        "    plt.legend(handles=legend_elements, loc='upper right',\n",
        "               bbox_to_anchor=(1.1, 1), ncol=1)\n",
        "\n",
        "    plt.title('Projection UMAP des prompts historiques par concept', fontsize=14)\n",
        "    plt.xlabel('UMAP Dimension 1', fontsize=12)\n",
        "    plt.ylabel('UMAP Dimension 2', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_path, 'umap_by_concept.png'), dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# Exemple d'utilisation\n",
        "# visualize_umap_by_concept(historical_prompts, historical_umap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpCXtfFezzb6"
      },
      "source": [
        "### 4. Nuage de mots pour chaque cluster\n",
        "\n",
        "Cette visualisation permet d'explorer les termes les plus fréquents dans chaque cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HKCYht_zzb7"
      },
      "outputs": [],
      "source": [
        "def generate_cluster_wordclouds(historical_prompts, output_path='./figures/'):\n",
        "    \"\"\"\n",
        "    Génère un nuage de mots pour chaque cluster identifié.\n",
        "\n",
        "    Args:\n",
        "        historical_prompts: DataFrame contenant les prompts historiques avec leurs clusters\n",
        "        output_path: Chemin pour sauvegarder les figures\n",
        "    \"\"\"\n",
        "    from wordcloud import WordCloud\n",
        "    import matplotlib.pyplot as plt\n",
        "    import os\n",
        "    import nltk\n",
        "    from nltk.corpus import stopwords\n",
        "\n",
        "    # Télécharger les stopwords si nécessaire\n",
        "    try:\n",
        "        nltk.data.find('corpora/stopwords')\n",
        "    except LookupError:\n",
        "        nltk.download('stopwords')\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    # Pour chaque cluster\n",
        "    for cluster_id in sorted(historical_prompts['cluster'].unique()):\n",
        "        if cluster_id == -1:  # Ignorer les points de bruit\n",
        "            continue\n",
        "\n",
        "        # Filtrer les prompts de ce cluster\n",
        "        cluster_prompts = historical_prompts[historical_prompts['cluster'] == cluster_id]['prompt']\n",
        "\n",
        "        if len(cluster_prompts) == 0:\n",
        "            continue\n",
        "\n",
        "        # Combiner tous les textes\n",
        "        text = ' '.join(cluster_prompts)\n",
        "\n",
        "        # Créer le nuage de mots\n",
        "        wordcloud = WordCloud(\n",
        "            width=800,\n",
        "            height=400,\n",
        "            background_color='white',\n",
        "            stopwords=stop_words,\n",
        "            max_words=100,\n",
        "            contour_width=3\n",
        "        ).generate(text)\n",
        "\n",
        "        # Afficher et sauvegarder\n",
        "        plt.figure(figsize=(16, 8))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Nuage de mots pour le Cluster {cluster_id} ({len(cluster_prompts)} prompts)',\n",
        "                 fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_path, f'wordcloud_cluster_{cluster_id}.png'), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    print(f\"Nuages de mots générés pour {len(historical_prompts['cluster'].unique()) - 1} clusters\")\n",
        "\n",
        "# Exemple d'utilisation\n",
        "# generate_cluster_wordclouds(historical_prompts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caLeAOPFzzb7"
      },
      "source": [
        "### 5. Distribution des concepts historiques par cluster\n",
        "Cette visualisation montre comment les différents concepts historiques sont distribués dans les clusters identifiés"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW_xK_49zzb8"
      },
      "outputs": [],
      "source": [
        "def visualize_concepts_by_cluster(historical_prompts, output_path='./figures/'):\n",
        "    \"\"\"\n",
        "    Crée une heatmap montrant la distribution des concepts historiques par cluster.\n",
        "\n",
        "    Args:\n",
        "        historical_prompts: DataFrame contenant les prompts historiques\n",
        "        output_path: Chemin pour sauvegarder les figures\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import pandas as pd\n",
        "    import os\n",
        "\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    # Ignorer les points de bruit (cluster -1) si présents\n",
        "    if -1 in historical_prompts['cluster'].unique():\n",
        "        df_filtered = historical_prompts[historical_prompts['cluster'] != -1].copy()\n",
        "    else:\n",
        "        df_filtered = historical_prompts.copy()\n",
        "\n",
        "    # Créer une table de contingence\n",
        "    cross_tab = pd.crosstab(\n",
        "        df_filtered['most_similar_concept'],\n",
        "        df_filtered['cluster'],\n",
        "        normalize='index'\n",
        "    )\n",
        "\n",
        "    # Tri pour une meilleure visualisation\n",
        "    # On trie les concepts par cluster dominant\n",
        "    dominant_clusters = cross_tab.idxmax(axis=1)\n",
        "    sorted_concepts = dominant_clusters.sort_values().index\n",
        "    cross_tab = cross_tab.loc[sorted_concepts]\n",
        "\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    sns.heatmap(cross_tab, annot=True, cmap=\"YlGnBu\", fmt='.0%')\n",
        "    plt.title('Distribution des concepts historiques par cluster', fontsize=16)\n",
        "    plt.ylabel('Concept historique', fontsize=14)\n",
        "    plt.xlabel('Cluster', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_path, 'concept_cluster_distribution.png'), dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# Exemple d'utilisation\n",
        "# visualize_concepts_by_cluster(historical_prompts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nj9wR-fzzb8"
      },
      "source": [
        "### 6. Analyse des termes les plus communs par concept historique\n",
        "\n",
        "Cette visualisation aide à comprendre quels termes sont les plus associés à chaque concept historique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGSJ72tAzzb8"
      },
      "outputs": [],
      "source": [
        "def analyze_terms_by_concept(historical_prompts, output_path='./figures/'):\n",
        "    \"\"\"\n",
        "    Analyse et visualise les termes les plus fréquents pour chaque concept historique.\n",
        "\n",
        "    Args:\n",
        "        historical_prompts: DataFrame contenant les prompts historiques\n",
        "        output_path: Chemin pour sauvegarder les figures\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import os\n",
        "    import nltk\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    from nltk.corpus import stopwords\n",
        "    from collections import Counter\n",
        "\n",
        "    # Télécharger les ressources NLTK nécessaires\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "        nltk.data.find('corpora/stopwords')\n",
        "    except LookupError:\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('stopwords')\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    # Pour chaque concept historique\n",
        "    for concept in historical_prompts['most_similar_concept'].unique():\n",
        "        # Filtrer les prompts de ce concept\n",
        "        concept_prompts = historical_prompts[historical_prompts['most_similar_concept'] == concept]['prompt']\n",
        "\n",
        "        # Combiner tous les prompts\n",
        "        text = ' '.join(concept_prompts)\n",
        "\n",
        "        # Tokenizer\n",
        "        tokens = word_tokenize(text.lower())\n",
        "\n",
        "        # Filtrer les stopwords et les tokens courts\n",
        "        filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words and len(word) > 2]\n",
        "\n",
        "        # Compter les occurrences\n",
        "        word_counts = Counter(filtered_tokens)\n",
        "\n",
        "        # Prendre les N mots les plus fréquents\n",
        "        top_n = 20\n",
        "        top_words = word_counts.most_common(top_n)\n",
        "\n",
        "        # Préparer les données pour le graphique\n",
        "        words = [word for word, count in top_words]\n",
        "        counts = [count for word, count in top_words]\n",
        "\n",
        "        # Créer le graphique\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.barh(words[::-1], counts[::-1], color='steelblue')\n",
        "        plt.xlabel('Fréquence')\n",
        "        plt.title(f'Termes les plus fréquents pour \"{concept}\" ({len(concept_prompts)} prompts)',\n",
        "                 fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_path, f'terms_{concept.replace(\" \", \"_\")}.png'), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    print(f\"Analyse des termes générée pour {len(historical_prompts['most_similar_concept'].unique())} concepts\")\n",
        "\n",
        "# Exemple d'utilisation\n",
        "# analyze_terms_by_concept(historical_prompts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kRRe0dgzzb9"
      },
      "source": [
        "### 7. Visualisation interactive avec Plotly\n",
        "\n",
        "Cette cellule crée une visualisation interactive de la projection UMAP qui permet d'explorer les prompts historiques de manière plus interactive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27WS7FxPzzb9"
      },
      "outputs": [],
      "source": [
        "def create_interactive_visualization(historical_prompts, historical_umap, output_path='./figures/'):\n",
        "    \"\"\"\n",
        "    Crée une visualisation interactive des prompts historiques avec Plotly.\n",
        "\n",
        "    Args:\n",
        "        historical_prompts: DataFrame contenant les prompts historiques\n",
        "        historical_umap: Coordonnées UMAP des prompts historiques\n",
        "        output_path: Chemin pour sauvegarder les figures\n",
        "    \"\"\"\n",
        "    import plotly.express as px\n",
        "    import pandas as pd\n",
        "    import os\n",
        "\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    # Créer un DataFrame pour Plotly\n",
        "    viz_df = pd.DataFrame({\n",
        "        'UMAP1': historical_umap[:, 0],\n",
        "        'UMAP2': historical_umap[:, 1],\n",
        "        'Cluster': historical_prompts['cluster'],\n",
        "        'Concept': historical_prompts['most_similar_concept'],\n",
        "        'Score': historical_prompts['similarity_score'],\n",
        "        'Prompt': historical_prompts['prompt']\n",
        "    })\n",
        "\n",
        "    # Créer la visualisation interactive\n",
        "    fig = px.scatter(\n",
        "        viz_df,\n",
        "        x='UMAP1',\n",
        "        y='UMAP2',\n",
        "        color='Concept',\n",
        "        hover_data=['Prompt', 'Score', 'Cluster'],\n",
        "        opacity=0.7,\n",
        "        title='Exploration interactive des prompts historiques',\n",
        "        template='plotly_white',\n",
        "        color_discrete_sequence=px.colors.qualitative.Bold\n",
        "    )\n",
        "\n",
        "    # Améliorer la mise en page\n",
        "    fig.update_layout(\n",
        "        legend=dict(\n",
        "            orientation=\"h\",\n",
        "            yanchor=\"bottom\",\n",
        "            y=-0.2,\n",
        "            xanchor=\"center\",\n",
        "            x=0.5\n",
        "        ),\n",
        "        width=1200,\n",
        "        height=800\n",
        "    )\n",
        "\n",
        "    # Enregistrer en tant que fichier HTML autonome\n",
        "    fig.write_html(os.path.join(output_path, 'interactive_visualization.html'))\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Exemple d'utilisation\n",
        "# fig = create_interactive_visualization(historical_prompts, historical_umap)\n",
        "# fig.show()  # Afficher dans le notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN3hWpg1zzb-"
      },
      "source": [
        "### 8. Réseau de co-occurrence de concepts dans les clusters\n",
        "Cette visualisation montre comment les concepts historiques sont liés entre eux à travers leur présence dans les mêmes clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwOIg5Qozzb_"
      },
      "outputs": [],
      "source": [
        "def visualize_concept_network(historical_prompts, output_path='./figures/'):\n",
        "    \"\"\"\n",
        "    Crée une visualisation en réseau des relations entre concepts historiques\n",
        "    basée sur leur co-occurrence dans les clusters.\n",
        "\n",
        "    Args:\n",
        "        historical_prompts: DataFrame contenant les prompts historiques\n",
        "        output_path: Chemin pour sauvegarder les figures\n",
        "    \"\"\"\n",
        "    import networkx as nx\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "    import os\n",
        "\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    # Filtrer pour exclure les points de bruit\n",
        "    if -1 in historical_prompts['cluster'].unique():\n",
        "        df_filtered = historical_prompts[historical_prompts['cluster'] != -1].copy()\n",
        "    else:\n",
        "        df_filtered = historical_prompts.copy()\n",
        "\n",
        "    # Créer un graphe\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Ajouter des nœuds pour chaque concept\n",
        "    concepts = df_filtered['most_similar_concept'].unique()\n",
        "    for concept in concepts:\n",
        "        count = df_filtered[df_filtered['most_similar_concept'] == concept].shape[0]\n",
        "        G.add_node(concept, size=count, count=count)\n",
        "\n",
        "    # Pour chaque cluster, créer des liens entre concepts présents\n",
        "    for cluster in df_filtered['cluster'].unique():\n",
        "        # Obtenir les concepts dans ce cluster\n",
        "        cluster_concepts = df_filtered[df_filtered['cluster'] == cluster]['most_similar_concept'].unique()\n",
        "\n",
        "        # Créer des liens pour chaque paire de concepts\n",
        "        for i, concept1 in enumerate(cluster_concepts):\n",
        "            for concept2 in cluster_concepts[i+1:]:\n",
        "                # Si le lien existe déjà, augmenter son poids\n",
        "                if G.has_edge(concept1, concept2):\n",
        "                    G[concept1][concept2]['weight'] += 1\n",
        "                else:\n",
        "                    G.add_edge(concept1, concept2, weight=1)\n",
        "\n",
        "    # Taille des nœuds basée sur la fréquence\n",
        "    node_sizes = [G.nodes[node]['size'] * 20 for node in G.nodes]\n",
        "\n",
        "    # Épaisseur des liens basée sur les poids\n",
        "    edge_weights = [G[u][v]['weight'] * 0.5 for u, v in G.edges]\n",
        "\n",
        "    # Positionner les nœuds\n",
        "    pos = nx.spring_layout(G, seed=42, k=0.3)\n",
        "\n",
        "    plt.figure(figsize=(14, 12))\n",
        "\n",
        "    # Dessiner les nœuds\n",
        "    nx.draw_networkx_nodes(G, pos,\n",
        "                          node_size=node_sizes,\n",
        "                          node_color='skyblue',\n",
        "                          alpha=0.8)\n",
        "\n",
        "    # Dessiner les liens\n",
        "    nx.draw_networkx_edges(G, pos,\n",
        "                          width=edge_weights,\n",
        "                          alpha=0.5,\n",
        "                          edge_color='gray')\n",
        "\n",
        "    # Ajouter les étiquettes\n",
        "    nx.draw_networkx_labels(G, pos, font_size=10, font_family='sans-serif')\n",
        "\n",
        "    plt.title('Réseau de co-occurrence des concepts historiques', fontsize=16)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_path, 'concept_network.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return G\n",
        "\n",
        "# Exemple d'utilisation\n",
        "concept_network = visualize_concept_network(historical_prompts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byOi5gGezzb_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "citation-manager": {
      "items": {}
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11 (negotiating_past)",
      "language": "python",
      "name": "negotiating_past"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "62c3170c2f24407abb87bc93afa5f063": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49068d3445b14602b76c14c92e50a409",
              "IPY_MODEL_a66c63f3ca934e99be65840c0f5b90e7",
              "IPY_MODEL_e043f8479bf1454885c3b317fe946142"
            ],
            "layout": "IPY_MODEL_7cc4f9a7ecdc462c920b32fd7d2ea5a7"
          }
        },
        "49068d3445b14602b76c14c92e50a409": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cb25dd06f45466bb960803f4dbb2840",
            "placeholder": "​",
            "style": "IPY_MODEL_8040b1cc982c45e28e83bbaf03451319",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "a66c63f3ca934e99be65840c0f5b90e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6222c98afbb44de881f2902f45f2247",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be2ccb548a9d4e518694c4e588388eb3",
            "value": 48
          }
        },
        "e043f8479bf1454885c3b317fe946142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_343ac99d91cd4adba46bdcc7b68766a0",
            "placeholder": "​",
            "style": "IPY_MODEL_6e2272babb314360a6cb04ed203e782c",
            "value": " 48.0/48.0 [00:00&lt;00:00, 2.20kB/s]"
          }
        },
        "7cc4f9a7ecdc462c920b32fd7d2ea5a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cb25dd06f45466bb960803f4dbb2840": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8040b1cc982c45e28e83bbaf03451319": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6222c98afbb44de881f2902f45f2247": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be2ccb548a9d4e518694c4e588388eb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "343ac99d91cd4adba46bdcc7b68766a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e2272babb314360a6cb04ed203e782c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d71516e36efb42859213dd8ef59e9884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4438105186b4733b5cf246db38e0c61",
              "IPY_MODEL_4f6d17d4bb8d44bfb652e1da461885a3",
              "IPY_MODEL_56e0845543d64ff0832a85535fc5a035"
            ],
            "layout": "IPY_MODEL_741afd0679a647288d8ba753514d3618"
          }
        },
        "e4438105186b4733b5cf246db38e0c61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ade41a469015466cb71427fadc712217",
            "placeholder": "​",
            "style": "IPY_MODEL_3c70c76956114640849a009014304979",
            "value": "vocab.txt: 100%"
          }
        },
        "4f6d17d4bb8d44bfb652e1da461885a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02e187d3ad624bf5815a39592e6f95a3",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c8cebba2d5448e885a4e3206158341e",
            "value": 231508
          }
        },
        "56e0845543d64ff0832a85535fc5a035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eece8b7ac86242aebfb9cffefa14b6d5",
            "placeholder": "​",
            "style": "IPY_MODEL_84df3ae560c44a8d92f2c98c43a4e0bc",
            "value": " 232k/232k [00:00&lt;00:00, 6.53MB/s]"
          }
        },
        "741afd0679a647288d8ba753514d3618": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ade41a469015466cb71427fadc712217": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c70c76956114640849a009014304979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02e187d3ad624bf5815a39592e6f95a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c8cebba2d5448e885a4e3206158341e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eece8b7ac86242aebfb9cffefa14b6d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84df3ae560c44a8d92f2c98c43a4e0bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4eddb4280ea94723b1568de69b3d96f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a487b7462ba4192b1ce4a327e91ad7a",
              "IPY_MODEL_22cd6800c2c64cb8b77d96d6ab01649e",
              "IPY_MODEL_89db48cc382749ee8f8ccb2c8a39e361"
            ],
            "layout": "IPY_MODEL_35bbe9ce17324b77b46de5a53ea3e51f"
          }
        },
        "9a487b7462ba4192b1ce4a327e91ad7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff3274e1c9e6415aab3e7a35d3a46c6b",
            "placeholder": "​",
            "style": "IPY_MODEL_7de7bbdcc624418a8ac714e55635caec",
            "value": "tokenizer.json: 100%"
          }
        },
        "22cd6800c2c64cb8b77d96d6ab01649e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d1d198cf4f0464ab4a55f3570404053",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_372ee47421d24971bddc0f5d47df6e28",
            "value": 466062
          }
        },
        "89db48cc382749ee8f8ccb2c8a39e361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e19d9ea7ed14c56a7bdc58aa06b6b31",
            "placeholder": "​",
            "style": "IPY_MODEL_c9ef3b3addda4bb6ae8adbaa4f6912ce",
            "value": " 466k/466k [00:00&lt;00:00, 2.58MB/s]"
          }
        },
        "35bbe9ce17324b77b46de5a53ea3e51f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff3274e1c9e6415aab3e7a35d3a46c6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7de7bbdcc624418a8ac714e55635caec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d1d198cf4f0464ab4a55f3570404053": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "372ee47421d24971bddc0f5d47df6e28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e19d9ea7ed14c56a7bdc58aa06b6b31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9ef3b3addda4bb6ae8adbaa4f6912ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cdfa8d8314a484984c31d7494fd6052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4482c1c111cd483c999d4c224ea0dd22",
              "IPY_MODEL_5e16b04e795b4db99c9458349e71f3f2",
              "IPY_MODEL_16d2daa80e3f45d6aa17821bd94b6634"
            ],
            "layout": "IPY_MODEL_5b3e0a4a19ea4681b1edf563d704364d"
          }
        },
        "4482c1c111cd483c999d4c224ea0dd22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_454e8640ef244b619f0fcfe7a092e000",
            "placeholder": "​",
            "style": "IPY_MODEL_52c7322437314574aa2ea75d5ba11cae",
            "value": "config.json: 100%"
          }
        },
        "5e16b04e795b4db99c9458349e71f3f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85dd4352a94d4913a74ec9821263c3b3",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04faf007a9cd49c7bb4e424f182934e9",
            "value": 483
          }
        },
        "16d2daa80e3f45d6aa17821bd94b6634": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9643879863a441cdb04fb47d707edbda",
            "placeholder": "​",
            "style": "IPY_MODEL_fc56808171464c25906f36c9c1efa45a",
            "value": " 483/483 [00:00&lt;00:00, 41.5kB/s]"
          }
        },
        "5b3e0a4a19ea4681b1edf563d704364d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "454e8640ef244b619f0fcfe7a092e000": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52c7322437314574aa2ea75d5ba11cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85dd4352a94d4913a74ec9821263c3b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04faf007a9cd49c7bb4e424f182934e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9643879863a441cdb04fb47d707edbda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc56808171464c25906f36c9c1efa45a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21f4c2f3b0834df980cbbdf6ed72f098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d64e53ba5ed642c9b134349e3ca77b06",
              "IPY_MODEL_32a7d9b907b5468fb6857bd10ce6d21e",
              "IPY_MODEL_45ccf3b24509415d8ed5f1126bb2610a"
            ],
            "layout": "IPY_MODEL_69ffd53a959140dca6051746fa01c491"
          }
        },
        "d64e53ba5ed642c9b134349e3ca77b06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5572b8459c5c459b8f32ef2f28c99cd0",
            "placeholder": "​",
            "style": "IPY_MODEL_74db9695956144aa96c90b364868125b",
            "value": "model.safetensors: 100%"
          }
        },
        "32a7d9b907b5468fb6857bd10ce6d21e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fbd871893974b19a97493a5c97ede04",
            "max": 267954768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ddeb11a5efc44ae299a4f245f96aeb9f",
            "value": 267954768
          }
        },
        "45ccf3b24509415d8ed5f1126bb2610a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a70411d94b594869be10c1c110e82c52",
            "placeholder": "​",
            "style": "IPY_MODEL_9e6be02d85114d938d415011d7b8bcf5",
            "value": " 268M/268M [00:01&lt;00:00, 184MB/s]"
          }
        },
        "69ffd53a959140dca6051746fa01c491": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5572b8459c5c459b8f32ef2f28c99cd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74db9695956144aa96c90b364868125b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fbd871893974b19a97493a5c97ede04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddeb11a5efc44ae299a4f245f96aeb9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a70411d94b594869be10c1c110e82c52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e6be02d85114d938d415011d7b8bcf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}