{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# negotiating the past"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the environment, please read README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This presentation explores the intersection of historical imagination, artificial intelligence, and collective memory. We'll examine how users \"negotiate\" with AI systems to express their conceptions of the past, and how these interactions can reveal tensions between user expectations and AI-embedded historical patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Theoretical Framework**: How LLMs encode historical perspectives \n",
    "2. **Methodological Approach**: Analyzing historical references in prompts\n",
    "3. **Results Analysis**: What prompt analysis reveals about historical imagination\n",
    "4. **Conclusion**: New spaces for historical negotiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Theoretical Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs and Embedded Historical Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LLMs as Statistical Pattern Recognizers\n",
    "- Training Data as Historical Record\n",
    "- Historical Biases in Language Models\n",
    "- \"Stochastic Parrots\" and Historical Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Foundation of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs rely on transformer architectures that predict tokens based on previous context. Their \"knowledge\" of history comes from statistical patterns in training data, not genuine understanding. This creates an interesting dynamic when users prompt these systems about historical topics - the system's responses reveal embedded historical narratives from their training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Knowledge in Vector Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word embeddings capture semantic relationships\n",
    "- Historical concepts represented as vectors\n",
    "- Temporal relationships encoded in semantic proximity\n",
    "- Cultural associations embedded in language patterns\n",
    "\n",
    "Within the vector space of LLMs, historical concepts are encoded as points in multidimensional space. The relationships between historical events, figures, and concepts are captured in the distances and directions between these vectors. These semantic relationships reflect collective memory patterns from the training corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collective Memory and LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LLMs as repositories of digitized collective memory\n",
    "- Training data selection as memory politics\n",
    "- The \"averaged\" nature of AI-generated historical narratives\n",
    "- Absence of contested memory in statistical consensus\n",
    "\n",
    "From a memory studies perspective, LLMs function as repositories of digitized collective memory. The selection of training data constitutes a form of memory politics, determining which historical perspectives are included or excluded. The statistical nature of these models produces \"averaged\" historical narratives that often elide contestation and complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Methodological Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge of Historical Prompt Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Beyond simple keyword approaches\n",
    "- Historical references: explicit vs. implicit\n",
    "- Temporality in language\n",
    "- Building a robust identification strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying prompts that reference history requires more sophisticated approaches than simple keyword matching. Historical references can be explicit (\"Napoleon Bonaparte\") or implicit (\"the Emperor's exile\"), and may involve complex temporal markers. Our methodology must capture this complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Historical Prompt Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    }
   ],
   "source": [
    "# Loading our dataset of 10 million prompts\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Make sure you have the necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Read the CSV file\n",
    "prompts_df = pd.read_csv(\"data/prompts.csv\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "        return tokens\n",
    "    return []\n",
    "\n",
    "# Apply preprocessing to create tokens\n",
    "prompts_df['tokens'] = prompts_df['prompt'].apply(preprocess)\n",
    "\n",
    "# Create phrases (bigrams and trigrams)\n",
    "phrases = Phrases(prompts_df['tokens'], min_count=5, threshold=10)\n",
    "bigram = Phraser(phrases)\n",
    "prompts_df['tokens_bigrams'] = prompts_df['tokens'].apply(lambda x: bigram[x])\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=prompts_df['tokens_bigrams'],\n",
    "    vector_size=50,  # Dimension of the word vectors\n",
    "    window=5,        # Context window size\n",
    "    min_count=5,     # Minimum word frequency\n",
    "    workers=4,       # Number of threads to run in parallel\n",
    "    sg=0,            # 0 for CBOW, 1 for skip-gram\n",
    "    epochs=20        # Number of iterations\n",
    ")\n",
    "\n",
    "# Get word vectors\n",
    "word_vectors = word2vec_model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our technical approach involves transforming prompts into vector representations to enable semantic analysis. By using text embedding techniques, we can identify prompts with historical references beyond simple keyword matching. This allows us to build a dataset that captures the rich variety of ways people reference the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Reference Detection Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Identify seed terms for historical references\n",
    "historical_seeds = [\"history\", \"ancient\", \"medieval\", \"renaissance\", \n",
    "                   \"revolution\", \"war\", \"empire\", \"century\", \"past\"]\n",
    "\n",
    "# Find semantically similar terms using word vectors\n",
    "def find_similar_terms(word_vectors, seed_terms, n=100):\n",
    "    similar_terms = set()\n",
    "    for term in seed_terms:\n",
    "        if term in word_vectors:\n",
    "            # Get similar words using the most_similar method from word2vec\n",
    "            similar_words = word_vectors.most_similar(term, topn=n)\n",
    "            # Extract just the words (not the similarity scores)\n",
    "            similar_terms.update([word for word, score in similar_words])\n",
    "    return list(similar_terms)\n",
    "\n",
    "historical_terms = find_similar_terms(word_vectors, historical_seeds, n=100)\n",
    "\n",
    "# Filter prompts containing historical references\n",
    "def contains_terms(text, terms):\n",
    "    if isinstance(text, str):\n",
    "        pattern = '|'.join(r'\\b{}\\b'.format(re.escape(term)) for term in terms)\n",
    "        return bool(re.search(pattern, text.lower()))\n",
    "    return False\n",
    "\n",
    "# Apply the filter\n",
    "historical_prompts = prompts_df[prompts_df['prompt'].apply(\n",
    "    lambda x: contains_terms(x, historical_terms))]\n",
    "\n",
    "# Alternatively, use cosine similarity between prompt embeddings and historical concepts\n",
    "# Load a sentence transformer model for text embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Get embeddings for all prompts\n",
    "prompt_embeddings = model.encode(prompts_df['prompt'].fillna('').tolist())\n",
    "\n",
    "# Get embeddings for historical terms (combine them into a representative text)\n",
    "historical_text = \" \".join(historical_terms)\n",
    "historical_concept_embedding = model.encode([historical_text])[0]\n",
    "\n",
    "# Calculate cosine similarity between each prompt and the historical concepts\n",
    "similarity_scores = cosine_similarity(\n",
    "    prompt_embeddings, \n",
    "    historical_concept_embedding.reshape(1, -1)\n",
    ")\n",
    "\n",
    "# Flatten the similarity scores array\n",
    "similarity_scores = similarity_scores.flatten()\n",
    "\n",
    "# Define threshold (you'll need to determine an appropriate value)\n",
    "threshold = 0.5  # Example value, adjust based on your data\n",
    "\n",
    "# Filter prompts with similarity scores above the threshold\n",
    "historical_prompts_alt = prompts_df[similarity_scores > threshold]\n",
    "\n",
    "print(f\"Number of historical prompts (keyword method): {len(historical_prompts)}\")\n",
    "print(f\"Number of historical prompts (embedding method): {len(historical_prompts_alt)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify prompts with historical references, we use both lexical and semantic approaches. Starting with seed historical terms, we expand to semantically similar concepts using word embeddings. We can then filter prompts either through string matching or by measuring the semantic similarity between prompts and historical concepts using cosine similarity of their vector representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Historical Imagination Through Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Make sure you have the necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Cluster historical prompts to identify themes\n",
    "# Assuming prompt_embeddings is already defined from the previous code\n",
    "\n",
    "# Apply UMAP for dimensionality reduction\n",
    "umap_model = UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "prompt_umap = umap_model.fit_transform(prompt_embeddings)\n",
    "\n",
    "# Apply DBSCAN for clustering\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "cluster_labels = dbscan.fit_predict(prompt_umap)\n",
    "\n",
    "# Add cluster labels to the dataframe\n",
    "historical_prompts['cluster'] = cluster_labels\n",
    "\n",
    "# Function to extract top words from a list of prompts\n",
    "def extract_top_words(prompts, n=10):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    all_words = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        if isinstance(prompt, str):\n",
    "            # Tokenize and filter out stopwords\n",
    "            words = [w.lower() for w in word_tokenize(prompt) \n",
    "                    if w.lower() not in stop_words and w.isalpha() and len(w) > 2]\n",
    "            all_words.extend(words)\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(all_words)\n",
    "    \n",
    "    # Return the top N words\n",
    "    return word_counts.most_common(n)\n",
    "\n",
    "# Extract common themes from clusters\n",
    "cluster_themes = []\n",
    "for c in sorted(historical_prompts['cluster'].unique()):\n",
    "    if c != -1:  # Skip noise points (cluster -1 in DBSCAN)\n",
    "        cluster_prompts = historical_prompts[historical_prompts['cluster'] == c]['prompt'].tolist()\n",
    "        top_words = extract_top_words(cluster_prompts, n=10)\n",
    "        cluster_themes.append({\n",
    "            'cluster': c,\n",
    "            'n_prompts': len(cluster_prompts),\n",
    "            'sample_prompts': cluster_prompts[:5],  # Take first 5 examples\n",
    "            'themes': top_words\n",
    "        })\n",
    "\n",
    "# Print cluster themes\n",
    "for theme in cluster_themes:\n",
    "    print(f\"Cluster {theme['cluster']} - {theme['n_prompts']} prompts\")\n",
    "    print(f\"Top themes: {', '.join([word for word, count in theme['themes']])}\")\n",
    "    print(f\"Sample prompts:\")\n",
    "    for i, prompt in enumerate(theme['sample_prompts']):\n",
    "        print(f\"  {i+1}. {prompt[:100]}...\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(\n",
    "    prompt_umap[:, 0], \n",
    "    prompt_umap[:, 1], \n",
    "    c=cluster_labels, \n",
    "    cmap='tab20', \n",
    "    alpha=0.6, \n",
    "    s=10\n",
    ")\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title('Clusters of Historical Prompts')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.savefig('historical_prompt_clusters.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've identified historically-related prompts, we use clustering techniques to discover common themes and patterns. By applying dimensionality reduction with UMAP and clustering with DBSCAN, we can identify groups of prompts that reference similar historical concepts, periods, or events. This allows us to map the landscape of historical imagination as expressed through user prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Results Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings: Historical References in Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Temporal distribution of historical references\n",
    "- Common historical personas, events, and eras\n",
    "- Stylistic patterns in historical prompts\n",
    "- Historical accuracy vs. creative liberty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis reveals several patterns in how users reference history in their prompts. We observe a distribution across different historical periods, with certain eras receiving more attention than others. Popular historical figures and events appear frequently, often with creative embellishments that reveal more about contemporary imagination than historical fact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conflicting Historical Narratives\n",
    "\n",
    "We observe interesting conflicts between user expectations and AI-generated content, particularly around contested historical narratives. Users often prompt for versions of history that align with their preconceptions, while AI systems may present different perspectives based on their training data. These negotiations reveal the tension between personal historical imagination and collective historical narratives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Negotiation Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt refinement strategies\n",
    "- Adjective use to guide historical tone\n",
    "- Specificity vs. generality in historical requests\n",
    "- \"Historical\" as stylistic marker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users employ various strategies to negotiate with AI systems when requesting historical content. They refine prompts through iteration, use specific adjectives to guide the tone of historical representation, and vary between highly specific historical requests and general period references. Many users also use \"historical\" as a stylistic marker rather than a request for historical accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: European Historical References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building on our previous analysis of prompts containing \"European Union,\" we observe how users connect contemporary European politics with various historical periods and concepts. References to empires, wars, and political movements reveal how users conceptualize European history in relation to current events. These connections offer insight into how collective memory shapes understanding of present political entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI as a New Space for Historical Negotiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparing with traditional sites of historical consensus\n",
    "- Public vs. private negotiation of historical understanding\n",
    "- The role of algorithms in mediating historical perspectives\n",
    "- Implications for collective memory formation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI systems represent a new space for the negotiation of historical understanding, distinct from traditional sites like educational curricula, museums, or truth commissions. Unlike these institutional spaces, AI interactions are often private, individualized, and algorithmically mediated. This raises important questions about how collective memory will form in an era where historical understanding is increasingly negotiated through technological interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Research Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Longitudinal analysis of historical prompts\n",
    "- Cross-cultural comparisons of historical references\n",
    "- Educational applications of prompt analysis\n",
    "- Ethical considerations for AI-mediated historical understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This research opens several promising directions for future work. Longitudinal studies could track how historical references in prompts evolve over time in response to current events. Cross-cultural analyses might reveal different patterns of historical reference across languages and regions. Educational applications could help develop more historically-informed AI systems. Finally, ethical considerations around AI's role in mediating historical understanding require careful attention."
   ]
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
