{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncks0uTDzzbr"
      },
      "source": [
        "# negotiating the past"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAMm3ta9zzbt"
      },
      "source": [
        "To install the environment, see README.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmRnFRyxzzb1"
      },
      "source": [
        "We load the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrCaiB6xzzb1",
        "outputId": "3be594f9-f6ce-4d62-9e35-6e942556cb4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: umap-learn in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (0.5.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from umap-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from umap-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from umap-learn) (1.6.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from umap-learn) (0.61.2)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from umap-learn) (4.67.1)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from numba>=0.51.2->umap-learn) (0.44.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from scikit-learn>=0.22->umap-learn) (3.6.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (3.9.1)\n",
            "Requirement already satisfied: click in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/negotiating_past/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "%pip install umap-learn\n",
        "%pip install gensim\n",
        "%pip install nltk\n",
        "%pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaKgggqHzzb2"
      },
      "source": [
        "## Creating a Historical Prompt Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX5tD0evzzb3",
        "outputId": "94ffeb73-1938-41ad-eaab-954e7c334b0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/frederic.clavert/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/frederic.clavert/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading the libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import ssl\n",
        "\n",
        "# Make sure you have the necessary NLTK resources\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### loading the original dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>portrait of an anime girl in white and golden ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>girl with super long hair, hair becoming autum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>scifi art by greg rutkowski, a very tall, and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>skinny nerdy poindexter white kid with glasses...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a city made entirely out of Rubik\\'s cubes, da...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ancient greek statues taking selfies and watch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>what if we will never meet again detailed anal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>the batmobile caught in the flow of time. octa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>screenshot of a smiling shirtless muscular sen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>short kid wearing a supreme shirt, detailed, s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              prompt\n",
              "0  portrait of an anime girl in white and golden ...\n",
              "1  girl with super long hair, hair becoming autum...\n",
              "2  scifi art by greg rutkowski, a very tall, and ...\n",
              "3  skinny nerdy poindexter white kid with glasses...\n",
              "4  a city made entirely out of Rubik\\'s cubes, da...\n",
              "5  ancient greek statues taking selfies and watch...\n",
              "6  what if we will never meet again detailed anal...\n",
              "7  the batmobile caught in the flow of time. octa...\n",
              "8  screenshot of a smiling shirtless muscular sen...\n",
              "9  short kid wearing a supreme shirt, detailed, s..."
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# si l'on veut un échantillon aléatoire.\n",
        "# Définir la taille de l'échantillon souhaité\n",
        "sample_size = 1000000\n",
        "# Déterminer le nombre total de lignes (pour les fichiers très grands)\n",
        "total_rows = sum(1 for _ in open('data/prompts.csv')) - 1  # -1 pour l'en-tête\n",
        "\n",
        "# Calculer la probabilité de sélection pour chaque ligne\n",
        "skip_prob = 1 - sample_size / total_rows\n",
        "\n",
        "# Utiliser le paramètre skiprows avec une fonction lambda\n",
        "prompts_df = pd.read_csv('data/prompts.csv', \n",
        "                        usecols=[0],  # Sélectionne uniquement la première colonne\n",
        "                        skiprows=lambda x: x > 0 and np.random.random() < skip_prob)\n",
        "\n",
        "# Si l'on veut juste charger tout le fichier -- attention 10 millions de lignes\n",
        "# prompts_df = pd.read_csv('data/prompts.csv', usecols=[0])\n",
        "\n",
        "# on vérifie ce que l'on a engendré:\n",
        "prompts_df.shape\n",
        "prompts_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWV0Mqlazzb3"
      },
      "source": [
        "### Historical Reference Detection Approach With DistilBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWAtuQpchZ_W"
      },
      "source": [
        "#### Set up the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "i7kCiBnizzb3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "from umap.umap_ import UMAP\n",
        "from sklearn.cluster import DBSCAN\n",
        "from tqdm import tqdm  # Pour les barres de progression\n",
        "import gc  # Nettoyage de mémoire\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Définir le nombre de cœurs CPU à utiliser\n",
        "num_cpu_cores = 8\n",
        "torch.set_num_threads(num_cpu_cores)\n",
        "os.environ[\"OMP_NUM_THREADS\"] = str(num_cpu_cores)\n",
        "os.environ[\"MKL_NUM_THREADS\"] = str(num_cpu_cores)\n",
        "\n",
        "def improved_historical_identification(prompts_df):\n",
        "    \"\"\"\n",
        "    Version améliorée de l'identification des prompts historiques\n",
        "    \"\"\"\n",
        "    # Échantillonnage si nécessaire\n",
        "    #if len(prompts_df) > sample_size:\n",
        "    #    sample_df = prompts_df.sample(sample_size, random_state=42)\n",
        "    #    print(f\"Échantillonnage de {sample_size} prompts sur {len(prompts_df)} au total\")\n",
        "    #else:\n",
        "    #    sample_df = prompts_df.copy()\n",
        "    #    print(f\"Traitement de tous les {len(sample_df)} prompts\")\n",
        "\n",
        "\n",
        "    # Liste des prompts avec filtrage des valeurs non valides\n",
        "    prompts_list = prompts_df['prompt'].fillna('').tolist()\n",
        "    valid_text_mask = [isinstance(text, str) and bool(text.strip()) for text in prompts_list]\n",
        "    valid_prompts = [text for i, text in enumerate(prompts_list) if valid_text_mask[i]]\n",
        "\n",
        "    print(f\"Prompts valides : {len(valid_prompts)} sur {len(prompts_list)}\")\n",
        "\n",
        "    # Filtrer le DataFrame pour ne conserver que les lignes avec texte valide\n",
        "    prompts_df = prompts_df[valid_text_mask].reset_index(drop=True)\n",
        "\n",
        "    # Génération des embeddings\n",
        "    print(\"Génération des embeddings des prompts...\")\n",
        "    print(\"Chargement du modèle SentenceTransformer...\")\n",
        "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "    # Traitement par lots pour économiser la mémoire\n",
        "    batch_size = 32\n",
        "    prompt_embeddings = []\n",
        "\n",
        "    for i in tqdm(range(0, len(valid_prompts), batch_size), desc=\"Génération des embeddings\"):\n",
        "        batch = valid_prompts[i:i+batch_size]\n",
        "        batch_embeddings = model.encode(batch, show_progress_bar=False)\n",
        "        prompt_embeddings.append(batch_embeddings)\n",
        "\n",
        "    prompt_embeddings = np.vstack(prompt_embeddings)\n",
        "\n",
        "    # Vérifier que la longueur correspond\n",
        "    assert len(prompts_df) == prompt_embeddings.shape[0], \"Incohérence entre la longueur du DataFrame et des embeddings\"\n",
        "\n",
        "    # Définir des concepts historiques plus diversifiés\n",
        "    historical_concepts = [\n",
        "        # on va s'intéresser aux guerres napoléoniennes\n",
        "        \"napoleon\", \"napoléon\", \"Napoléon\", \"Napoleon\", \"Bonaparte\", \"bonaparte\", \"napoleonic\", \"Napoleonic\", \"lest we forget\"\n",
        "        # Périodes historiques explicites\n",
        "        # \"ancient history\", \"medieval times\", \"renaissance period\",\n",
        "        # \"world war\", \"cold war\", \"industrial revolution\", \"prehistoric era\",\n",
        "\n",
        "        # Termes plus généraux et implicites\n",
        "        # \"history\", \"historical event\", \"in the past\", \"ancient times\",\n",
        "        # \"vintage\", \"retro\", \"old fashioned\", \"traditional\",\n",
        "\n",
        "        # Styles et esthétiques historiques\n",
        "        # \"victorian style\", \"art deco\", \"baroque\", \"gothic\", \"classical\",\n",
        "        # \"midcentury\", \"ancient greek\", \"roman empire\", \"medieval knight\",\n",
        "\n",
        "        # Références à la culture populaire historique\n",
        "        # \"steampunk\", \"dieselpunk\", \"historical fiction\", \"period drama\",\n",
        "        # \"historical costume\", \"historical setting\"\n",
        "    ]\n",
        "\n",
        "    # Générer des embeddings pour les concepts historiques\n",
        "    print(\"Génération des embeddings de concepts historiques...\")\n",
        "    historical_embeddings = model.encode(historical_concepts)\n",
        "\n",
        "    # Calculer la similarité pour chaque concept\n",
        "    print(\"Calcul des similarités...\")\n",
        "    concept_similarities = np.zeros((len(valid_prompts), len(historical_concepts)))\n",
        "\n",
        "    for i in tqdm(range(0, len(valid_prompts), batch_size), desc=\"Calcul par lots\"):\n",
        "        end_idx = min(i + batch_size, len(valid_prompts))\n",
        "        batch_embeddings = prompt_embeddings[i:end_idx]\n",
        "\n",
        "        for j in range(len(historical_concepts)):\n",
        "            concept_similarities[i:end_idx, j] = cosine_similarity(\n",
        "                batch_embeddings,\n",
        "                historical_embeddings[j].reshape(1, -1)\n",
        "            ).flatten()\n",
        "\n",
        "    # Pour chaque prompt, trouver le concept le plus similaire\n",
        "    max_similarity_indices = np.argmax(concept_similarities, axis=1)\n",
        "    max_similarities = np.max(concept_similarities, axis=1)\n",
        "\n",
        "    # Attribuer des seuils différents selon les catégories de concepts\n",
        "    concept_thresholds = {\n",
        "        # Seuils plus élevés pour termes explicites\n",
        "        \"lest we forget\": 0.5,\n",
        "        \"napoleon\": 0.5,\n",
        "        \"napoléon\": 0.5,\n",
        "        \"Napoléon\": 0.5,\n",
        "        \"Napoleon\": 0.5,\n",
        "        \"Bonaparte\": 0.5,\n",
        "        \"bonaparte\": 0.5,\n",
        "        \"napoleonic\": 0.5,\n",
        "        \"Napoleonic\": 0.5\n",
        "        #\"ancient history\": 0.5,\n",
        "        #\"medieval times\": 0.5,\n",
        "        #\"renaissance period\": 0.5,\n",
        "        #\"world war\": 0.5,\n",
        "        #\"cold war\": 0.5,\n",
        "        #\"industrial revolution\": 0.5,\n",
        "\n",
        "        # Seuils plus bas pour termes implicites ou généraux\n",
        "        #\"history\": 0.45,\n",
        "        #\"historical event\": 0.45,\n",
        "        #\"in the past\": 0.4,\n",
        "        #\"ancient times\": 0.45,\n",
        "\n",
        "        # Seuils intermédiaires pour styles\n",
        "        #\"victorian style\": 0.48,\n",
        "        #\"art deco\": 0.48,\n",
        "        #\"baroque\": 0.48,\n",
        "    }\n",
        "\n",
        "    # Seuil par défaut pour les concepts non listés spécifiquement\n",
        "    default_threshold = 0.45\n",
        "\n",
        "    # Vérifier si chaque prompt dépasse le seuil pour son concept le plus similaire\n",
        "    is_historical = []\n",
        "    for i, (max_sim, concept_idx) in enumerate(zip(max_similarities, max_similarity_indices)):\n",
        "        concept = historical_concepts[concept_idx]\n",
        "        threshold = concept_thresholds.get(concept, default_threshold)\n",
        "        is_historical.append(max_sim > threshold)\n",
        "\n",
        "    # Convertir en array numpy pour faciliter l'indexation\n",
        "    is_historical = np.array(is_historical)\n",
        "\n",
        "    # Ajouter les résultats au DataFrame\n",
        "    prompts_df['most_similar_concept'] = [historical_concepts[idx] for idx in max_similarity_indices]\n",
        "    prompts_df['similarity_score'] = max_similarities\n",
        "    prompts_df['is_historical'] = is_historical\n",
        "\n",
        "    # Filtrer les prompts historiques\n",
        "    historical_prompts = prompts_df[is_historical].copy()\n",
        "\n",
        "    print(f\"Identification de {len(historical_prompts)} prompts historiques ({len(historical_prompts)/len(prompts_df):.2%} de l'échantillon)\")\n",
        "\n",
        "    # Méthode hybride - combiner embeddings et mots-clés pour améliorer la couverture\n",
        "    keywords = [\n",
        "         \"napoleon\", \"napoléon\", \"Napoléon\", \"Napoleon\", \"Bonaparte\", \"bonaparte\", \"napoleonic\", \"Napoleonic\", \"lest we forget\"\n",
        "#        \"history\", \"ancient\", \"medieval\", \"renaissance\", \"century\",\n",
        "#        \"war\", \"empire\", \"kingdom\", \"historical\", \"vintage\", \"retro\",\n",
        "#        \"traditional\", \"old\", \"classical\", \"antique\"\n",
        "    ]\n",
        "\n",
        "    keyword_mask = prompts_df['prompt'].fillna('').str.lower().str.contains('|'.join(keywords))\n",
        "    missed_by_embedding = prompts_df[~is_historical & keyword_mask].copy()\n",
        "    missed_by_embedding['detection_method'] = 'keyword_only'\n",
        "\n",
        "    # Ajouter un marqueur pour les prompts détectés par embedding\n",
        "    historical_prompts['detection_method'] = 'embedding'\n",
        "\n",
        "    # Fusionner les deux ensembles\n",
        "    combined_historical = pd.concat([historical_prompts, missed_by_embedding], ignore_index=True)\n",
        "\n",
        "    print(f\"Identification supplémentaire de {len(missed_by_embedding)} prompts par mots-clés\")\n",
        "    print(f\"Total de prompts historiques: {len(combined_historical)} ({len(combined_historical)/len(prompts_df):.2%} de l'échantillon)\")\n",
        "\n",
        "    return combined_historical, prompt_embeddings, is_historical\n",
        "\n",
        "def analyze_historical_clusters(historical_prompts, prompt_embeddings, is_historical):\n",
        "    \"\"\"\n",
        "    Version adaptée pour fonctionner avec la nouvelle méthode d'identification\n",
        "    \"\"\"\n",
        "    if len(historical_prompts) > 10:  # Besoin d'un minimum d'échantillons pour le clustering\n",
        "        # Sélectionner les embeddings des prompts historiques identifiés par embedding\n",
        "        embedding_historical = historical_prompts[historical_prompts['detection_method'] == 'embedding']\n",
        "\n",
        "        # Créer un masque pour extraire les embeddings appropriés\n",
        "        # Note: Cette approche suppose que les indices des historical_prompts correspondent\n",
        "        # aux indices filtrés dans prompt_embeddings\n",
        "        historical_indices = embedding_historical.index\n",
        "        historical_embeddings = prompt_embeddings[historical_indices]\n",
        "\n",
        "        print(\"Application de la réduction dimensionnelle UMAP...\")\n",
        "        # Appliquer UMAP avec optimisation CPU\n",
        "        umap_model = UMAP(n_neighbors=15, min_dist=0.1, random_state=42,\n",
        "                          n_jobs=num_cpu_cores)\n",
        "        historical_umap = umap_model.fit_transform(historical_embeddings)\n",
        "\n",
        "        print(\"Clustering avec DBSCAN...\")\n",
        "        # Appliquer DBSCAN\n",
        "        dbscan = DBSCAN(eps=0.5, min_samples=5, n_jobs=num_cpu_cores)\n",
        "        cluster_labels = dbscan.fit_predict(historical_umap)\n",
        "\n",
        "        # Ajouter les étiquettes de cluster au DataFrame, uniquement pour les prompts détectés par embedding\n",
        "        # Initialiser une colonne de cluster avec des valeurs NaN\n",
        "        historical_prompts['cluster'] = float('nan')\n",
        "\n",
        "        # Mettre à jour les valeurs de cluster pour les prompts détectés par embedding\n",
        "        idx_map = {old_idx: new_idx for new_idx, old_idx in enumerate(historical_indices)}\n",
        "        for i, idx in enumerate(historical_indices):\n",
        "            historical_prompts.loc[idx, 'cluster'] = cluster_labels[i]\n",
        "\n",
        "        # Générer rapport et visualisations, uniquement pour les prompts avec clusters\n",
        "        generate_cluster_report(embedding_historical, historical_umap, cluster_labels)\n",
        "\n",
        "        return historical_prompts, historical_umap, cluster_labels\n",
        "    else:\n",
        "        print(\"Pas assez de prompts historiques pour le clustering\")\n",
        "        return historical_prompts, None, None\n",
        "\n",
        "def generate_cluster_report(historical_prompts, historical_umap, cluster_labels):\n",
        "    # Nombre de prompts par cluster\n",
        "    cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
        "    print(\"\\nNombre de prompts par cluster:\")\n",
        "    print(cluster_counts)\n",
        "\n",
        "    # Visualiser les clusters\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    # Exclure les points de bruit (cluster -1) pour une meilleure visualisation\n",
        "    non_noise_mask = cluster_labels != -1\n",
        "\n",
        "    if np.any(non_noise_mask):  # Vérifier qu'il y a des points non-bruit\n",
        "        scatter = plt.scatter(\n",
        "            historical_umap[non_noise_mask, 0],\n",
        "            historical_umap[non_noise_mask, 1],\n",
        "            c=cluster_labels[non_noise_mask],\n",
        "            cmap='tab20',\n",
        "            alpha=0.6,\n",
        "            s=10\n",
        "        )\n",
        "        plt.colorbar(scatter, label='Cluster')\n",
        "    else:\n",
        "        # S'il n'y a que des points de bruit, tous les afficher\n",
        "        plt.scatter(\n",
        "            historical_umap[:, 0],\n",
        "            historical_umap[:, 1],\n",
        "            c='gray',\n",
        "            alpha=0.3,\n",
        "            s=5\n",
        "        )\n",
        "\n",
        "    plt.title('Clusters des Prompts Historiques (méthode améliorée)')\n",
        "    plt.xlabel('UMAP Dimension 1')\n",
        "    plt.ylabel('UMAP Dimension 2')\n",
        "    plt.savefig('historical_prompt_clusters_improved.png', dpi=300, bbox_inches='tight')\n",
        "#    plt.close()  # Fermer la figure pour libérer la mémoire\n",
        "\n",
        "    # Exemples de prompts de chaque cluster\n",
        "    print(\"\\nExemples de prompts par cluster:\")\n",
        "    for cluster_id in sorted(set([label for label in cluster_labels if label != -1])):\n",
        "        # Créer un masque pour ce cluster\n",
        "        cluster_mask = cluster_labels == cluster_id\n",
        "\n",
        "        # Trouver les indices correspondants dans historical_prompts\n",
        "        cluster_indices = np.where(cluster_mask)[0]\n",
        "\n",
        "        # Récupérer les lignes correspondantes\n",
        "        if len(cluster_indices) > 0:\n",
        "            # Échantillonner jusqu'à 5 prompts de ce cluster\n",
        "            sample_indices = np.random.choice(cluster_indices, min(5, len(cluster_indices)), replace=False)\n",
        "\n",
        "            print(f\"\\nCluster {cluster_id} ({len(cluster_indices)} prompts):\")\n",
        "\n",
        "            for idx in sample_indices:\n",
        "                row_idx = historical_prompts.index[idx]\n",
        "                row = historical_prompts.loc[row_idx]\n",
        "                print(f\"  - {row['prompt'][:100]}... (Similarité: {row['similarity_score']:.2f}, Concept: {row['most_similar_concept']})\")\n",
        "\n",
        "# Fonction principale pour exécuter l'ensemble du pipeline\n",
        "def run_historical_prompt_analysis(prompts_df):\n",
        "    # Identification des prompts historiques avec la méthode améliorée\n",
        "    historical_prompts, prompt_embeddings, is_historical = improved_historical_identification(prompts_df)\n",
        "\n",
        "    # Analyse de clusters sur les prompts historiques identifiés\n",
        "    historical_prompts_clustered, umap_result, cluster_labels = analyze_historical_clusters(\n",
        "        historical_prompts, prompt_embeddings, is_historical)\n",
        "\n",
        "    return historical_prompts_clustered, umap_result, cluster_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### we export the clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vQVTbUVMDtez"
      },
      "outputs": [],
      "source": [
        "# we also export the CSVs\n",
        "\n",
        "def export_clusters_to_csv(historical_prompts, output_dir=\"data/generated/cluster_exports\"):\n",
        "    \"\"\"\n",
        "    Exporte les prompts regroupés par cluster dans des fichiers CSV distincts.\n",
        "    Chaque fichier contient les prompts, scores de similarité et concepts associés.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    historical_prompts : pandas.DataFrame\n",
        "        DataFrame contenant les prompts historiques avec les colonnes:\n",
        "        - prompt: texte du prompt\n",
        "        - similarity_score: score de similarité\n",
        "        - most_similar_concept: concept historique le plus similaire\n",
        "        - cluster: étiquette de cluster (-1 pour les points de bruit)\n",
        "        - detection_method: méthode ayant détecté le prompt (embedding ou keyword_only)\n",
        "\n",
        "    output_dir : str\n",
        "        Répertoire de sortie pour les fichiers CSV\n",
        "    \"\"\"\n",
        "    # Créer le répertoire de sortie s'il n'existe pas\n",
        "    import os\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "        print(f\"Répertoire créé: {output_dir}\")\n",
        "\n",
        "    # Extraire les prompts avec clusters valides (détection par embedding)\n",
        "    clustered_prompts = historical_prompts[historical_prompts['cluster'].notna()].copy()\n",
        "\n",
        "    # Ajouter une version tronquée du prompt pour la prévisualisation\n",
        "    clustered_prompts['prompt_preview'] = clustered_prompts['prompt'].apply(\n",
        "        lambda x: x[:100] + '...' if len(x) > 100 else x\n",
        "    )\n",
        "\n",
        "    # Créer un fichier récapitulatif pour tous les clusters\n",
        "    summary_file = os.path.join(output_dir, \"all_clusters_summary.csv\")\n",
        "    clusters_summary = clustered_prompts.groupby('cluster').agg(\n",
        "        prompt_count=('prompt', 'count'),\n",
        "        avg_similarity=('similarity_score', 'mean'),\n",
        "        top_concepts=('most_similar_concept', lambda x: x.value_counts().index[0] if len(x) > 0 else 'N/A'),\n",
        "        min_similarity=('similarity_score', 'min'),\n",
        "        max_similarity=('similarity_score', 'max')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Ajouter des exemples de prompts au résumé (3 exemples par cluster)\n",
        "    def get_sample_prompts(group):\n",
        "        if len(group) <= 3:\n",
        "            return '; '.join(group['prompt_preview'].tolist())\n",
        "        else:\n",
        "            return '; '.join(group.sample(3)['prompt_preview'].tolist())\n",
        "\n",
        "    sample_prompts = clustered_prompts.groupby('cluster').apply(get_sample_prompts).reset_index()\n",
        "    sample_prompts.columns = ['cluster', 'sample_prompts']\n",
        "    clusters_summary = pd.merge(clusters_summary, sample_prompts, on='cluster')\n",
        "\n",
        "    # Enregistrer le résumé\n",
        "    clusters_summary.to_csv(summary_file, index=False, encoding='utf-8')\n",
        "    print(f\"Résumé des clusters enregistré dans {summary_file}\")\n",
        "\n",
        "    # Exporter chaque cluster vers un fichier CSV distinct\n",
        "    unique_clusters = clustered_prompts['cluster'].unique()\n",
        "\n",
        "    for cluster_id in unique_clusters:\n",
        "        # Ignorer les points de bruit pour l'exportation individuelle\n",
        "        if cluster_id == -1:\n",
        "            noise_file = os.path.join(output_dir, \"noise_points.csv\")\n",
        "            noise_prompts = clustered_prompts[clustered_prompts['cluster'] == -1]\n",
        "            if not noise_prompts.empty:\n",
        "                noise_prompts.to_csv(noise_file, index=False, encoding='utf-8')\n",
        "                print(f\"Points de bruit ({len(noise_prompts)}) enregistrés dans {noise_file}\")\n",
        "            continue\n",
        "\n",
        "        # Filtrer les prompts de ce cluster\n",
        "        cluster_df = clustered_prompts[clustered_prompts['cluster'] == cluster_id].copy()\n",
        "\n",
        "        # Trier par similarité décroissante\n",
        "        cluster_df = cluster_df.sort_values('similarity_score', ascending=False)\n",
        "\n",
        "        # Analyser les concepts présents dans ce cluster\n",
        "        concept_counts = cluster_df['most_similar_concept'].value_counts()\n",
        "        top_concepts = concept_counts.head(3).to_dict()  # Top 3 concepts\n",
        "\n",
        "        # Créer un fichier avec un nom informatif\n",
        "        top_concept = concept_counts.index[0] if len(concept_counts) > 0 else \"inconnu\"\n",
        "        top_concept_safe = ''.join(c if c.isalnum() else '_' for c in top_concept)  # Nom de fichier sécurisé\n",
        "\n",
        "        file_name = f\"cluster_{int(cluster_id):02d}_{top_concept_safe}_{len(cluster_df)}_prompts.csv\"\n",
        "        file_path = os.path.join(output_dir, file_name)\n",
        "\n",
        "        # Enregistrer ce cluster en CSV\n",
        "        cluster_df.to_csv(file_path, index=False, encoding='utf-8')\n",
        "        print(f\"Cluster {int(cluster_id)} ({top_concept}, {len(cluster_df)} prompts) enregistré dans {file_name}\")\n",
        "\n",
        "    # Exporter les prompts détectés uniquement par mots-clés\n",
        "    keyword_only = historical_prompts[historical_prompts['detection_method'] == 'keyword_only']\n",
        "    if not keyword_only.empty:\n",
        "        keyword_file = os.path.join(output_dir, \"keyword_only_prompts.csv\")\n",
        "        keyword_only.to_csv(keyword_file, index=False, encoding='utf-8')\n",
        "        print(f\"Prompts détectés par mots-clés uniquement ({len(keyword_only)}) enregistrés dans {keyword_file}\")\n",
        "\n",
        "    # Créer un fichier HTML pour explorer les clusters\n",
        "    create_html_explorer(historical_prompts, os.path.join(output_dir, \"cluster_explorer.html\"))\n",
        "\n",
        "    return clusters_summary\n",
        "\n",
        "def create_html_explorer(historical_prompts, output_file):\n",
        "    \"\"\"\n",
        "    Crée un fichier HTML simple pour explorer les clusters de prompts.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    historical_prompts : pandas.DataFrame\n",
        "        DataFrame contenant les prompts historiques avec clustering\n",
        "    output_file : str\n",
        "        Chemin vers le fichier HTML de sortie\n",
        "    \"\"\"\n",
        "    # Extraire uniquement les prompts avec clusters valides\n",
        "    clustered_prompts = historical_prompts[historical_prompts['cluster'].notna()].copy()\n",
        "\n",
        "    # Récupérer la liste des clusters uniques (triés)\n",
        "    unique_clusters = sorted(clustered_prompts['cluster'].unique())\n",
        "\n",
        "    # Calculer des statistiques pour chaque cluster\n",
        "    cluster_stats = {}\n",
        "    for cluster_id in unique_clusters:\n",
        "        if cluster_id == -1:  # Traiter les points de bruit séparément\n",
        "            continue\n",
        "\n",
        "        cluster_df = clustered_prompts[clustered_prompts['cluster'] == cluster_id]\n",
        "        concept_counts = cluster_df['most_similar_concept'].value_counts().head(3)\n",
        "\n",
        "        cluster_stats[cluster_id] = {\n",
        "            'size': len(cluster_df),\n",
        "            'avg_similarity': cluster_df['similarity_score'].mean(),\n",
        "            'top_concepts': dict(concept_counts),\n",
        "            'samples': cluster_df.sort_values('similarity_score', ascending=False).head(5)['prompt'].tolist()\n",
        "        }\n",
        "\n",
        "    # Générer le HTML\n",
        "    html_content = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html lang=\"en\">\n",
        "    <head>\n",
        "        <meta charset=\"UTF-8\">\n",
        "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "        <title>Explorateur de Clusters de Prompts Historiques</title>\n",
        "        <style>\n",
        "            body { font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }\n",
        "            h1 { color: #2c3e50; }\n",
        "            h2 { color: #3498db; margin-top: 30px; }\n",
        "            .cluster { border: 1px solid #ddd; padding: 15px; margin: 15px 0; border-radius: 5px; }\n",
        "            .cluster-header { display: flex; justify-content: space-between; }\n",
        "            .concept-tag { display: inline-block; background: #e0f7fa; padding: 3px 8px; margin: 3px; border-radius: 3px; }\n",
        "            .prompt-item { padding: 8px; margin: 5px 0; background: #f9f9f9; border-left: 3px solid #3498db; }\n",
        "            .similarity { font-size: 0.8em; color: #666; }\n",
        "            .stats { color: #7f8c8d; font-size: 0.9em; }\n",
        "            .toggle-button { background: #3498db; color: white; border: none; padding: 5px 10px; cursor: pointer; border-radius: 3px; }\n",
        "            .prompt-list { max-height: 0; overflow: hidden; transition: max-height 0.3s ease-out; }\n",
        "            .expanded { max-height: 2000px; }\n",
        "            .search-container { margin: 20px 0; }\n",
        "            #searchInput { padding: 8px; width: 250px; }\n",
        "            #statsSection { margin-top: 30px; }\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>Explorateur de Clusters de Prompts Historiques</h1>\n",
        "\n",
        "        <div class=\"search-container\">\n",
        "            <input type=\"text\" id=\"searchInput\" placeholder=\"Rechercher des prompts...\">\n",
        "            <button onclick=\"searchPrompts()\">Rechercher</button>\n",
        "        </div>\n",
        "\n",
        "        <div id=\"statsSection\">\n",
        "            <h2>Statistiques générales</h2>\n",
        "            <p>Nombre total de clusters: <strong>\"\"\" + str(len(cluster_stats)) + \"\"\"</strong></p>\n",
        "            <p>Nombre total de prompts clusterisés: <strong>\"\"\" + str(len(clustered_prompts)) + \"\"\"</strong></p>\n",
        "        </div>\n",
        "\n",
        "        <div id=\"clustersSection\">\n",
        "    \"\"\"\n",
        "\n",
        "    # Ajouter chaque cluster au HTML\n",
        "    for cluster_id in sorted([c for c in unique_clusters if c != -1]):\n",
        "        stats = cluster_stats[cluster_id]\n",
        "\n",
        "        # Créer les balises de concepts\n",
        "        concept_tags = \"\"\n",
        "        for concept, count in stats['top_concepts'].items():\n",
        "            concept_tags += f'<span class=\"concept-tag\">{concept} ({count})</span>'\n",
        "\n",
        "        # Ajouter les prompts échantillons\n",
        "        prompt_items = \"\"\n",
        "        for i, prompt in enumerate(stats['samples']):\n",
        "            # Raccourcir le prompt pour l'affichage\n",
        "            display_prompt = prompt[:200] + \"...\" if len(prompt) > 200 else prompt\n",
        "            prompt_items += f'<div class=\"prompt-item\">{i+1}. {display_prompt}</div>'\n",
        "\n",
        "        # Créer la section de cluster\n",
        "        html_content += f\"\"\"\n",
        "        <div class=\"cluster\" data-cluster-id=\"{int(cluster_id)}\">\n",
        "            <div class=\"cluster-header\">\n",
        "                <h3>Cluster {int(cluster_id)}</h3>\n",
        "                <button class=\"toggle-button\" onclick=\"togglePrompts(this)\">Afficher les prompts</button>\n",
        "            </div>\n",
        "            <div class=\"stats\">\n",
        "                <p><strong>Taille:</strong> {stats['size']} prompts | <strong>Similarité moyenne:</strong> {stats['avg_similarity']:.2f}</p>\n",
        "                <p><strong>Concepts principaux:</strong> {concept_tags}</p>\n",
        "            </div>\n",
        "            <div class=\"prompt-list\">\n",
        "                <h4>Exemples de prompts:</h4>\n",
        "                {prompt_items}\n",
        "            </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "    # Ajouter les points de bruit\n",
        "    noise_df = clustered_prompts[clustered_prompts['cluster'] == -1]\n",
        "    if len(noise_df) > 0:\n",
        "        noise_samples = noise_df.sample(min(5, len(noise_df)))['prompt'].tolist()\n",
        "        noise_items = \"\"\n",
        "        for i, prompt in enumerate(noise_samples):\n",
        "            display_prompt = prompt[:200] + \"...\" if len(prompt) > 200 else prompt\n",
        "            noise_items += f'<div class=\"prompt-item\">{i+1}. {display_prompt}</div>'\n",
        "\n",
        "        html_content += f\"\"\"\n",
        "        <div class=\"cluster\">\n",
        "            <div class=\"cluster-header\">\n",
        "                <h3>Points de bruit</h3>\n",
        "                <button class=\"toggle-button\" onclick=\"togglePrompts(this)\">Afficher les prompts</button>\n",
        "            </div>\n",
        "            <div class=\"stats\">\n",
        "                <p><strong>Taille:</strong> {len(noise_df)} prompts</p>\n",
        "            </div>\n",
        "            <div class=\"prompt-list\">\n",
        "                <h4>Exemples de prompts:</h4>\n",
        "                {noise_items}\n",
        "            </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "    # Ajouter le JavaScript et fermer les balises HTML\n",
        "    html_content += \"\"\"\n",
        "        </div>\n",
        "\n",
        "        <script>\n",
        "            function togglePrompts(button) {\n",
        "                const list = button.parentNode.nextElementSibling.nextElementSibling;\n",
        "                list.classList.toggle('expanded');\n",
        "                button.textContent = list.classList.contains('expanded') ? 'Masquer les prompts' : 'Afficher les prompts';\n",
        "            }\n",
        "\n",
        "            function searchPrompts() {\n",
        "                const searchTerm = document.getElementById('searchInput').value.toLowerCase();\n",
        "                const clusters = document.querySelectorAll('.cluster');\n",
        "\n",
        "                if (searchTerm === '') {\n",
        "                    // Si la recherche est vide, afficher tous les clusters\n",
        "                    clusters.forEach(cluster => {\n",
        "                        cluster.style.display = 'block';\n",
        "                    });\n",
        "                    return;\n",
        "                }\n",
        "\n",
        "                clusters.forEach(cluster => {\n",
        "                    const promptItems = cluster.querySelectorAll('.prompt-item');\n",
        "                    let matchFound = false;\n",
        "\n",
        "                    promptItems.forEach(item => {\n",
        "                        if (item.textContent.toLowerCase().includes(searchTerm)) {\n",
        "                            matchFound = true;\n",
        "                        }\n",
        "                    });\n",
        "\n",
        "                    // Aussi vérifier dans les concepts\n",
        "                    const conceptTags = cluster.querySelectorAll('.concept-tag');\n",
        "                    conceptTags.forEach(tag => {\n",
        "                        if (tag.textContent.toLowerCase().includes(searchTerm)) {\n",
        "                            matchFound = true;\n",
        "                        }\n",
        "                    });\n",
        "\n",
        "                    cluster.style.display = matchFound ? 'block' : 'none';\n",
        "                });\n",
        "            }\n",
        "        </script>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    # Écrire le HTML dans un fichier\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(html_content)\n",
        "\n",
        "    print(f\"Explorateur HTML créé: {output_file}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSUX_ZVphfsW"
      },
      "source": [
        "### Run the pipline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YkAkm77RHuQq",
        "outputId": "b1b72118-7144-4d1c-9646-002838b1883e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompts valides : 988927 sur 998862\n",
            "Génération des embeddings des prompts...\n",
            "Chargement du modèle SentenceTransformer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Génération des embeddings:  33%|███▎      | 10304/30904 [2:00:17<4:14:30,  1.35it/s] "
          ]
        }
      ],
      "source": [
        "# Exécuter l'analyse et utiliser directement le résultat\n",
        "results, _, _ = run_historical_prompt_analysis(prompts_df)\n",
        "\n",
        "# Exporter avec les résultats\n",
        "summary = export_clusters_to_csv(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# et l'on regarde ce qu'il y a en mémoire\n",
        "%whos DataFrame"
      ]
    }
  ],
  "metadata": {
    "citation-manager": {
      "items": {}
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "negotiating_past",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
